<!doctype html><html lang=de dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Automatisierte Inhaltsanalyse & Kategorisierung von QAnon-Tweets | Matthias Zaugg</title><meta name=keywords content="Autoritärer Charakter,Quantitative Textanalyse,Rechtsextremismus"><meta name=description content="Die COVID-19-Pandemie führte zu einer ansteigenden Verbreitung der rechtsextremen Verschwörungstheorie QAnon. In diesem Beitrag wird dies anhand von Twitterdaten empirisch überprüft. Zusätzlich wird mit einer nonparametrischen, automatisierten Inhaltsanalyse  (readme2) eingeschätzt, wieviele der Tweets sich  affirmativ bzw. kritisch auf QAnon beziehen."><meta name=author content="Matthias Zaugg"><link rel=canonical href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/><meta name=google-site-verification content="G-WE9N8RLG8G"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://nomaad.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nomaad.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nomaad.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nomaad.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nomaad.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=de href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/"><meta property="og:site_name" content="Matthias Zaugg"><meta property="og:title" content="Automatisierte Inhaltsanalyse & Kategorisierung von QAnon-Tweets"><meta property="og:description" content="Die COVID-19-Pandemie führte zu einer ansteigenden Verbreitung der rechtsextremen Verschwörungstheorie QAnon. In diesem Beitrag wird dies anhand von Twitterdaten empirisch überprüft. Zusätzlich wird mit einer nonparametrischen, automatisierten Inhaltsanalyse  (readme2) eingeschätzt, wieviele der Tweets sich  affirmativ bzw. kritisch auf QAnon beziehen."><meta property="og:locale" content="de"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-08-18T00:00:00+00:00"><meta property="article:modified_time" content="2020-08-18T00:00:00+00:00"><meta property="article:tag" content="Autoritärer Charakter"><meta property="article:tag" content="Quantitative Textanalyse"><meta property="article:tag" content="Rechtsextremismus"><meta name=twitter:card content="summary"><meta name=twitter:title content="Automatisierte Inhaltsanalyse & Kategorisierung von QAnon-Tweets"><meta name=twitter:description content="Die COVID-19-Pandemie führte zu einer ansteigenden Verbreitung der rechtsextremen Verschwörungstheorie QAnon. In diesem Beitrag wird dies anhand von Twitterdaten empirisch überprüft. Zusätzlich wird mit einer nonparametrischen, automatisierten Inhaltsanalyse  (readme2) eingeschätzt, wieviele der Tweets sich  affirmativ bzw. kritisch auf QAnon beziehen."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nomaad.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Automatisierte Inhaltsanalyse \u0026 Kategorisierung von QAnon-Tweets","item":"https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Automatisierte Inhaltsanalyse \u0026 Kategorisierung von QAnon-Tweets","name":"Automatisierte Inhaltsanalyse \u0026 Kategorisierung von QAnon-Tweets","description":"Die COVID-19-Pandemie führte zu einer ansteigenden Verbreitung der rechtsextremen Verschwörungstheorie QAnon. In diesem Beitrag wird dies anhand von Twitterdaten empirisch überprüft. Zusätzlich wird mit einer nonparametrischen, automatisierten Inhaltsanalyse  (readme2) eingeschätzt, wieviele der Tweets sich  affirmativ bzw. kritisch auf QAnon beziehen.","keywords":["Autoritärer Charakter","Quantitative Textanalyse","Rechtsextremismus"],"articleBody":" Die durch die COVID-19-Pandemie verursachte globale Unsicherheit, führte zu einer zunehmenden Verbreitung von Verschwörungstheorien, darunter die rechtsextreme und trumpistische Verschwörungstheorie QAnon1.\nIn diesem Beitrag wird untersucht, ob sich solche Tendenzen in Twitter-Daten erkennen lassen. Dazu wurden Tweets gesammelt, welche sich auf QAnon beziehen und während des Lockdowns abgesetzt wurden. Basierend auf diesem Dataset erfolgt einerseits eine Gegenüberstellung zu der Zunahme von COVID-19-Fällen im gleichen Zeitraum. Andererseits wird mittels Machine Learning durch eine nonparametrische, automatisierte Inhaltsanalyse mit readme2 eine Einschätzung darüber abgegeben, wieviele der Tweets sich affirmativ bzw. kritisch auf QAnon beziehen.\nDie Twitter-Daten Das Dataset besteht aus 11’363 deutschsprachigen Tweets aus dem Zeitraum 1. Februar 2020 bis 24. April 2020, welche sich entweder durch das Schlagwort “QAnon” oder “wwg1wga” (ein Leitspruch und Codewort der Anhänger*innen, stehend für “where we go one, we go all” - der autoritäre Charakter kondensiert in einen Hashtag?!) auf QAnon beziehen. Diese wurden mit dem Python-Skript TweetScraper aggregiert, in einer lokalen MongoDB-Datenbank gespeichert und nach der Codierung (siehe unten) als JSON-Dump exportiert (hier.\nZunehmende COVID-19-Fälle vs. Anzahl QAnon-Tweets Zunächst soll überprüft werden, ob in den Daten tatsächlich eine Zunahme von QAnon-Tweets während der Corona-Pandemie feststellbar ist. Dafür werden die Anzahl Tweets pro Tag der Anzahl COVID-19-Fälle in der Schweiz, Deutschland und Österreich im gleichen Zeitraum gegenübergestellt.\nLesen wir die Daten ein:\n# Load the data from dump-file tweets \u003c- jsonlite::stream_in(file(\"qanon_dump.json\"), verbose = F) Für die Zeitstrahl-Analyse werden die Tweets nun nach Veröffentlichkeitsdatum geordnet und gruppiert:\n# Prepare data for timeline analysis tweets_uncoded \u003c- tweets %\u003e% filter(is.na(code)) %\u003e% # Filter out coded tweets - the dates of these got messed up while coding unfortunately.. mutate( datetime = datetime %\u003e% parse_date_time(orders = ' %Y-%m-%d %H%M%S') # Parse date ) %\u003e% mutate(datetime = datetime + 1*60*60) %\u003e% # Set time from UTC to CET. mutate(datetime = datetime %\u003e% round(units = 'days') %\u003e% as.POSIXct()) # Remove the time, we just need the dates # What timerange do we have? # tweets_uncoded %\u003e% pull(datetime) %\u003e% min() # tweets_uncoded %\u003e% pull(datetime) %\u003e% max() # Group by day tweet_groups \u003c- tweets_uncoded %\u003e% group_by(datetime) %\u003e% tally() %\u003e% rename(date = datetime) Mit dem R-Package tidycovid19 können wir ein Dataset mit der Anzahl COVID-19-Fällen laden und diese ebenfalls nach Datum gruppieren. Da nur Tweets aus dem deutssprachigen Raum analysiert werden, berücksichtigen wir auch nur Fallzahlen aus Deutschland, Österreich und der Schweiz. Dabei werden die COVID-19-Fallzahlen für die Visualisierung proportional zur Anzahl Tweets herunterskaliert - die Messeinheit der Y-Achse stimmt also in der Grafik nur für die Anzahl Tweets, nicht aber für die COVID-19-Fallzahlen!\n# Get covid-cases via tidycovid19 package # covid \u003c- download_merged_data(cached = TRUE) covid \u003c- jsonlite::stream_in(file(\"covid_data.json\"), verbose = F) # German cases covidDE \u003c- covid %\u003e% filter(iso3c == \"DEU\") %\u003e% select(date, confirmed) %\u003e% mutate(confirmed = round(confirmed/382)) %\u003e% # Normalize case numbers proportionaly to the maximum numbers of tweets (max (max 400/day) mutate( date = date %\u003e% # Parse date. parse_date_time(orders = ' %Y-%m-%d') ) %\u003e% filter(date \u003e \"2020-02-01\" \u0026 date \u003c\"2020-04-25\") # Swiss cases covidCH \u003c- covid %\u003e% filter(iso3c == \"CHE\") %\u003e% select(date, confirmed) %\u003e% mutate(confirmed = round(confirmed/71)) %\u003e% # Normalize case numbers proportionaly to the maximum numbers of tweets (max 400/day) mutate( date = date %\u003e% # Parse date. parse_date_time(orders = ' %Y-%m-%d') ) %\u003e% filter(date \u003e \"2020-02-01\" \u0026 date \u003c\"2020-04-25\") # Austrian cases covidAT \u003c- covid %\u003e% filter(iso3c == \"AUT\") %\u003e% select(date, confirmed) %\u003e% mutate(confirmed = round(confirmed/37)) %\u003e% # Normalize case numbers proportionaly to the maximum numbers of tweets (max 400/day) mutate( date = date %\u003e% # Parse date. parse_date_time(orders = ' %Y-%m-%d') ) %\u003e% filter(date \u003e \"2020-02-01\" \u0026 date \u003c\"2020-04-25\") # join tweets \u0026 covid-cases tweets_covid \u003c- covidDE %\u003e% right_join(covidCH, by=\"date\") %\u003e% right_join(covidAT, by=\"date\") %\u003e% right_join(tweet_groups, by=\"date\") %\u003e% select(date, Covid_DE = confirmed.x, Covid_CH = confirmed.y, Covid_AUT = confirmed, Num_Tweets=n) # rename columns for plotly Die nach Tag gruppierte Anzahl Tweets und COVID-19-Fälle können nun visualisiert werden:\n# \"Melt\" the dates for plotting melted \u003c- melt(tweets_covid, id=\"date\") # Prepare plot plot \u003c- ggplot(data=melted, aes(x=date, y=value, colour=variable)) + geom_line() + labs(title = 'Anzahl QAnon-Tweets vs. COVID-19-Fälle', y = 'Anzahl Tweets', x = 'Datum', subtitle = str_c(\"Total 11'363 Tweets aus dem Zeitraum 01.02.2020-24.04.2020\", \"(COVID-19-Fälle sind normalisiert, Y-Achse misst nur Anzahl Tweets)\"), colour = \"\" ) # plot it fig \u003c- ggplotly(plot) fig Der Plot zeigt deutlich, wie mit steigenden COVID-19-Fallzahlen auch die Anzahl abgesetzter Tweets zur QAnon-Verschwörungstheorie zunahmen. Lässt sich daraus schliessen, dass QAnon Anhänger*innen gewonnen hat? Nicht unbedingt. Die Tweets können sich auch kritisch auf QAnon beziehen und zum Beispiel über die Gefahren der Verschwörungstheorie aufklären wollen.\nIn einem weiteren Schritt soll nun durch maschinelles Lernen eine Schätzung versucht werden, wieviele Tweets sich prozentual entweder positiv auf QAnon beziehen und die Verschwörungstheorie verbreiten oder negativ und kritisch auf QAnon hinweisen.\nAutomatische Inhaltsanalyse mit dem R-Package readme2 Für die maschinelle Kategorisierung kommt hier das Package readme2 zum Einsatz, welches einen Machine-Learning-Algorithmus für die automatische Inhaltsanalyse von Texten für die Sozialwissenschaften implementiert.\nWie andere supervised ML-Algorithmen benötigt readme2 ein Trainingsset von Daten, mit welchen der Algorithmus trainiert wird. Dieses Trainingsset muss manuell erstellt werden, indem zunächst ein Codierschema festgelegt und dann die Tweets mit einem Code der entsprechenden Kategorie zugeordnet werden. Sobald der Algorithmus trainiert ist, kann damit via Spracherkennung eine Schätzung der Verteilung aller Kategorien in den unkategorisierten Tweets gemacht werden.\nManuelle Codierung der QAnon-Tweets Da ich zur Erstellung des Testsets eine relativ grosse Anzahl Tweets manuell codieren musste (ca. 10% der 11’363 Tweets) und keine Software fand, mit welcher sich das einfach und schnell bewerkstelligen lässt, habe ich dafür eine kleine Web-App entwickelt (Github-Link). Das Tool lädt zufällig einzelne Tweets aus dem gesamten Datenset, welche sich dann codieren lassen. Der Code wird dem Datensatz als Feld hinzugefügt und einer MongoDB Datenbank gespeichert. Damit wurden 904 Tweets jeweils einer von drei Kategorien zugeordnet (0 = nicht zuordenbar, 1 = kritisch-negativer Bezug, 2 = affirmativ-positiver Bezug). Disclaimer: Ich habe die Tweets in relativ schnellem Tempo alleine ohne Validierung codiert, es können also durchaus Fehlcodierungen vorhanden sein.\nTraining des readme2-Algorithmus Mit den codierten Tweets lässt sich nun der readme2-Algorithmus trainieren. Zur Textanalyse greift readme2 auf word embeddings zurück, welche mit dem GloVe-Algorithmus erstellt wurden. readme2 kommt mit englischen Worteinbettungen, ich habe deshalb für diese Analyse zunächst die deutschen Worteinbettungen von deepset.ai heruntergeladen, welche auf der deutschen Wikipedia trainiert wurden.\n# Select the tweets which have a code tweets_coded \u003c- tweets %\u003e% filter(!is.na(code)) # Helper function to load word embeddings loadVecs \u003c- function(path){ wordVecs_corpus \u003c- data.table::fread(path) wordVecs_keys \u003c- wordVecs_corpus[[1]]## first row is the name of the term wordVecs_corpus \u003c- as.matrix ( wordVecs_corpus[,-1] ) # row.names(wordVecs_corpus) \u003c- wordVecs_keys wordVecs \u003c- wordVecs_corpus rm(wordVecs_corpus) rm(wordVecs_keys) ## Remove the original loaded table to save space saveRDS(wordVecs, file = \"wordVecs.rds\") return(wordVecs) } ## Generate a word vector summary for each document # Use the german Wikipedia-trained GloVe word embeddings from https://deepset.ai/german-word-embeddings # Load word embeddings.. # wordVecs \u003c-loadVecs('deepset.ai.german.wikipedia.glove.txt') # wordVec_summaries = undergrad(documentText = cleanme(tweets_coded$text), wordVecs = wordVecs) #saveRDS(wordVec_summaries, file = \"wordVec_summaries.rds\") # ..or load from cache instead wordVec_summaries \u003c- readRDS(file = \"wordVec_summaries.rds\") Mit den deutschen Vektoren können 74% der Wörter in den 904 Tweets zugeordnet werden. Das scheint mir eher wenig, hat aber wohl damit zu tun, dass in den Tweets sehr viele themenspezifische Hashtags zu finden sind. Nun folgt 1) das eigentliche Training, 2) die automatisierte Kategorisierung der Testdaten und 3) der Abgleich zur Überprüfung mit den manuellen Codes der Testdaten. Schritt 1 und 2 werden direkt von der readme()-Funktion implementiert. Davor werden die 904 Tweets per Zufall in ein Test- und ein Trainingsset aufgeteilt. Ich mache drei Durchläufe mit jeweils unterschiedlichen Test- und Trainingssets, um einen besseren Eindruck über die Zuverlässigkeit der Schätzungen zu bekommen.\n# Evaluate 3 times, how accurate the estimations are with the coded trainingset use_virtualenv(\"r-tensorflow\") foreach(i=1:3) %do% { set.seed(i*123) # 1. Split coded tweets into a test and a training set rnd_train \u003c- sample(c(0,1), nrow(tweets_coded), replace = T) tweets_coded$trainingset \u003c- c(rnd_train) #length(which(tweets_coded$trainingset == 1)) #length(which(tweets_coded$trainingset == 0)) # 2. Call readme to make an estimation readme.estimates \u003c- readme(dfm = wordVec_summaries , labeledIndicator = tweets_coded$trainingset, categoryVec = tweets_coded$code) # 3. Compare estimates \u0026 actual values # Output proportions estimate estimate \u003c- readme.estimates$point_readme actual \u003c- table(tweets_coded$code[tweets_coded$trainingset == 0])/sum(table((tweets_coded$code[tweets_coded$trainingset == 0]))) # Calculate deviation of estimation from actual value in percent points percentages \u003c- ((actual - estimate)/actual) * 100 } ## TensorFlow v2.16.2 (~/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow) ## Python v3.10 (~/.virtualenvs/r-tensorflow/bin/python) ## [1] \"Performance warning: Rebuilding tensorflow graph...\" ## [1] \"Building master readme graph...\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.02 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.02 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## TensorFlow v2.16.2 (~/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow) ## Python v3.10 (~/.virtualenvs/r-tensorflow/bin/python) ## [1] \"Done with this round of training in 0.02 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## TensorFlow v2.16.2 (~/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow) ## Python v3.10 (~/.virtualenvs/r-tensorflow/bin/python) ## [1] \"Done with this round of training in 0.02 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.02 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.02 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [1] \"Done with this round of training in 0.01 minutes!\" ## [[1]] ## ## 0 1 2 ## -5.994926 -52.033951 10.389316 ## ## [[2]] ## ## 0 1 2 ## -1.591461 -40.736835 7.467267 ## ## [[3]] ## ## 0 1 2 ## -1.358362 -30.341873 5.887147 Geschätzte prozentuale Anteile:\nestimate ## 0 1 2 ## 0.2352962 0.1512897 0.6134141 Tatsächliche prozentuale Anteile:\nactual ## ## 0 1 2 ## 0.2321429 0.1160714 0.6517857 Abweichung der Schätzung vom tatsächlichen Anteil in Prozent:\npercentages ## ## 0 1 2 ## -1.358362 -30.341873 5.887147 Es zeigt sich nach drei Durchgängen, dass mit unseren Trainingsdaten die zweite Kategorie (QAnon verbreitend/affirmativ) unterschätzt, die erste Kategorie (QAnon kritisierend/negativ) jedoch überschätzt wird. Auch die Kategorie null (nicht zuordenbar) wird tendenziell unterschätzt. Für die Interpretation ist dies bei der automatischen Kategorisierung der gesamten 11’363 Tweets zu berücksichtigen.\nAutomatisierte Schätzung der Tweet-Kategorien Nach diesem Training kann nun eine Schätzung über sämtliche Tweets versucht werden. Dazu muss erst sämtlicher Text gegen die GloVe-Embeddings analysiert werden.\n# Apply to whole dataset tweets_all \u003c- tweets %\u003e% mutate(trainingset = ifelse(!is.na(code), 1, ifelse(is.na(code), 0, NA))) # Auskommentiert, da unten von Cache gelesend wird #wordVec_summaries_all = undergrad(documentText = cleanme(tweets_all$text), wordVecs = wordVecs) Die unkategorisierten Tweets werden nun in einem Loop für jeden Tag im gesamten Zeitraum aufgesplittet. Für jeden Tag wird dann readme() aufgerufen. Schlussendlich kann so visualisiert werden, ob und wie sich die Anteile der einzelnen Kategorien mit fortschreitendem zeitlichen Verlauf verändert haben. Da die Berechnung auf meinem Rechner mehr als 3 Stunden in Anspruch genommen hat, habe ich die Resultate in eine JSON-Datei gespeichert und lese für dieses RMarkdown nur die exportierte Datei, statt nochmals alles durchrechnen zu lassen.\n# Parse datetime tweets_date_sorted \u003c- tweets_all %\u003e% mutate( datetime = datetime %\u003e% parse_date_time(orders = ' %Y-%m-%d %H%M%S') # Parse date. ) %\u003e% mutate(datetime = datetime + 1*60*60) %\u003e% # Set time from UTC to CET. filter(!is.na(datetime)) %\u003e%# Remove messed up dates.. mutate(datetime = datetime %\u003e% round(units = 'days') %\u003e% as.POSIXct()) # Remove the time ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `datetime = datetime %\u003e% parse_date_time(orders = \" %Y-%m-%d ## %H%M%S\")`. ## Caused by warning: ## ! 412 failed to parse. # Update coded tweets with trainingset-field tweets_coded \u003c- tweets_all %\u003e% filter(trainingset == 1) # Group by day tweet_groups \u003c- tweets_date_sorted %\u003e% group_by(datetime) %\u003e% tally() # We will do estimates per day, therefore let's encapsulate the estimation in a function estimate_categories \u003c- function(i){ # Get the tweets for this date tweets_daily \u003c- tweets_date_sorted %\u003e% filter(datetime == tweet_groups[i,]$datetime) # Merge the daily tweets with the coded tweets from all dates - we need these for training.. tweet_set \u003c- merge(tweets_daily, tweets_coded, all=TRUE) # Calculate the wordvecs for the tweets for this date wordVec_summaries_set = undergrad(documentText = cleanme(tweet_set$text), wordVecs = wordVecs) # Make the estimation estimate \u003c- readme(dfm = wordVec_summaries_set , labeledIndicator = tweet_set$trainingset, categoryVec = tweet_set$code, verbose = T, diagnostics = T) # Return result return(estimate) } # Create a container for the results per day estimates \u003c- data.frame(date = tweet_groups[1,]$datetime, estimate = readme.estimates$point_readme[1], code = 1, stringsAsFactors = F) # Caching the results - The algorithm takes about 3-4 hours, I will not let it run again for the RMarkdown file.. estimate_file \u003c- \"qanon_readme_estimates_2020-02-01_2020_04_24.json\" if(!file.exists(estimate_file)){ # Calculate estimates. careful: takes ages! (+3h) foreach(i=2:nrow(tweet_groups)) %do% { # Call the function estimate \u003c- estimate_categories(i) # Add results to container estimates \u003c- estimates %\u003e% add_row(date = tweet_groups[i,]$datetime, estimate = estimate$point_readme[1], code = 0) estimates \u003c- estimates %\u003e% add_row(date = tweet_groups[i,]$datetime, estimate = estimate$point_readme[2], code = 1) estimates \u003c- estimates %\u003e% add_row(date = tweet_groups[i,]$datetime, estimate = estimate$point_readme[3], code = 2) } # Remove the first record that was created just for the initialization estimates \u003c- estimates[-1,] plotable \u003c- estimates %\u003e% pivot_wider(names_from = code, values_from = estimate) # Use pivot_wider from tidyr-Package to make data tidy # Export results as JSON file jsonlite::stream_out(plotable, file(estimate_file), verbose = F) } else{ # If file exists, load from file plotable \u003c- jsonlite::stream_in(file(\"qanon_readme_estimates_2020-02-01_2020_04_24.json\"), verbose = F) plotable \u003c- plotable %\u003e% mutate( date = date %\u003e% # Parse date. parse_date_time(orders = ' %Y-%m-%d') ) } plotable %\u003c\u003e% select(Datum = \"date\", N_A = \"0\", Kritisch = \"1\", Affirmativ = \"2\") # rename columns # \"Melt\" the dates.. long \u003c- melt(plotable, id=\"Datum\") # prepare plot plot \u003c- ggplot(data=long, aes(x=Datum, y=value, colour=variable)) + geom_line() + labs(title = 'Automatische Kategorisierung von QAnon-Tweets', y = 'Prozentualer Anteil', x = 'Datum', subtitle = str_c(\"Total 11'363 Tweets aus dem Zeitraum 01.02.2020-24.04.2020,\", \"Schätzung durch readme2-Algorithmus\"), colour = \"Kategorien\" ) #plot fig \u003c- ggplotly(plot) fig Im Plot wird ersichtlich, dass mit der automatischen Schätzung tendenziell eine Abnahme der affirmativen Verbreitung der QAnon-Verschwörungstheorie bei gleichzeitiger Zunahme der kritischen Stimmen festzustellen ist. Gemäss diesen Daten begann eine kritische Auseinandersetzung während der zunehmenden Verbreitung der neuen Verschwörungstheorie erst mit einer gewissen Verzögerung.\nFazit Es zeigt sich, dass eine insgesamt quantitative Zunahme eines bestimmten verschwörungstheoretischen Begriffs nicht nur affirmativ bzgl. der Verschwörungstheorie sein muss, sondern immer auch Gegenstimmen beinhaltet. Für fundiertere Aussagen müsste jedoch die Datenbasis genauer überprüft werden. Da die Tweets über die Schlagworte “QAnon” und “wwg1wga” gescraped wurden und zweiterer Begriff wohl vor allem von Befürworter:innen als Codewort verwendet wird, sind die Daten auch möglicherweise in Richtung Affirmation hin verzerrt Auch könnte die Wortzuordnung der word embeddings akkurater sein, vielleicht wäre mit einer Textbereinigung ein besseres Resultat erreichbar. Ausserdem hat sich beim Training gezeigt, dass die affirmative Kategorie systematisch unterschätzt wurde. Da müsste genauer hingeschaut werden. Zu untersuchen wäre auch, wie sich die Verteilung im weiteren Verlauf bis heute entwickelt hat. Dies dürfte sich nun als schwieriger erweisen, da Twitter unterdessen gegen die Verbreitung von QAnon-Inhalten vorgeht.\nSiehe dazu z.B. “QAnon” – der Aufstieg einer gefährlichen Verschwörungstheorie, Redaktionsnetzwerk Deutschland, 1. April 2020, abgerufen am 18. August 2020 oder Die Verschwörungsfanatiker von QAnon., Der SPIEGEL, 4. August 2020, abgerufen am 18. August 2020 ↩︎\n","wordCount":"2841","inLanguage":"de","datePublished":"2020-08-18T00:00:00Z","dateModified":"2020-08-18T00:00:00Z","author":{"@type":"Person","name":"Matthias Zaugg"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/"},"publisher":{"@type":"Organization","name":"Matthias Zaugg","logo":{"@type":"ImageObject","url":"https://nomaad.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nomaad.github.io/ accesskey=h title="Matthias Zaugg (Alt + H)"><img src=https://nomaad.github.io/apple-touch-icon.png alt aria-label=logo height=35>Matthias Zaugg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nomaad.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nomaad.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nomaad.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Automatisierte Inhaltsanalyse & Kategorisierung von QAnon-Tweets</h1><div class=post-meta><span title='2020-08-18 00:00:00 +0000 UTC'>18. August 2020</span>&nbsp;·&nbsp;14 Minuten&nbsp;·&nbsp;2841 Wörter&nbsp;·&nbsp;Matthias Zaugg</div></header><div class=post-content><link href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/htmltools-fill/fill.css rel=stylesheet><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/htmlwidgets/htmlwidgets.js></script><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/plotly-binding/plotly.js></script><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/typedarray/typedarray.min.js></script><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/jquery/jquery.min.js></script><link href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/crosstalk/css/crosstalk.min.css rel=stylesheet><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/crosstalk/js/crosstalk.min.js></script><link href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css rel=stylesheet><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/plotly-main/plotly-latest.min.js></script><link href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/htmltools-fill/fill.css rel=stylesheet><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/htmlwidgets/htmlwidgets.js></script><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/plotly-binding/plotly.js></script><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/typedarray/typedarray.min.js></script><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/jquery/jquery.min.js></script><link href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/crosstalk/css/crosstalk.min.css rel=stylesheet><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/crosstalk/js/crosstalk.min.js></script><link href=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css rel=stylesheet><script src=https://nomaad.github.io/posts/2020-08-17-categorize-qanon-references-with-machine-learning/index_files/plotly-main/plotly-latest.min.js></script><p>Die durch die COVID-19-Pandemie verursachte globale Unsicherheit, führte zu einer zunehmenden Verbreitung von Verschwörungstheorien, darunter die rechtsextreme und trumpistische Verschwörungstheorie <a href=https://de.wikipedia.org/wiki/QAnon>QAnon</a><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>In diesem Beitrag wird untersucht, ob sich solche Tendenzen in Twitter-Daten erkennen lassen. Dazu wurden Tweets gesammelt, welche sich auf QAnon beziehen und während des Lockdowns abgesetzt wurden. Basierend auf diesem Dataset erfolgt einerseits eine Gegenüberstellung zu der Zunahme von COVID-19-Fällen im gleichen Zeitraum. Andererseits wird mittels Machine Learning durch eine nonparametrische, automatisierte Inhaltsanalyse mit <a href=https://github.com/iqss-research/readme-software>readme2</a> eine Einschätzung darüber abgegeben, wieviele der Tweets sich affirmativ bzw. kritisch auf QAnon beziehen.</p><h3 id=die-twitter-daten>Die Twitter-Daten<a hidden class=anchor aria-hidden=true href=#die-twitter-daten>#</a></h3><p>Das Dataset besteht aus 11’363 deutschsprachigen Tweets aus dem Zeitraum 1. Februar 2020 bis 24. April 2020, welche sich entweder durch das Schlagwort “QAnon” oder “wwg1wga” (ein Leitspruch und Codewort der Anhänger*innen, stehend für “where we go one, we go all” - der autoritäre Charakter kondensiert in einen Hashtag?!) auf QAnon beziehen. Diese wurden mit dem Python-Skript <a href=https://github.com/jonbakerfish/TweetScraper>TweetScraper</a> aggregiert, in einer lokalen <a href=https://www.mongodb.com/>MongoDB</a>-Datenbank gespeichert und nach der Codierung (siehe unten) als JSON-Dump exportiert (<a href=qanon_dump.json>hier</a>.</p><h2 id=zunehmende-covid-19-fälle-vsanzahl-qanon-tweets>Zunehmende COVID-19-Fälle vs. Anzahl QAnon-Tweets<a hidden class=anchor aria-hidden=true href=#zunehmende-covid-19-fälle-vsanzahl-qanon-tweets>#</a></h2><p>Zunächst soll überprüft werden, ob in den Daten tatsächlich eine Zunahme von QAnon-Tweets während der Corona-Pandemie feststellbar ist. Dafür werden die Anzahl Tweets pro Tag der Anzahl COVID-19-Fälle in der Schweiz, Deutschland und Österreich im gleichen Zeitraum gegenübergestellt.</p><p>Lesen wir die Daten ein:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Load the data from dump-file</span>
</span></span><span class=line><span class=cl><span class=n>tweets</span> <span class=o>&lt;-</span> <span class=n>jsonlite</span><span class=o>::</span><span class=nf>stream_in</span><span class=p>(</span><span class=nf>file</span><span class=p>(</span><span class=s>&#34;qanon_dump.json&#34;</span><span class=p>),</span> <span class=n>verbose</span> <span class=o>=</span> <span class=bp>F</span><span class=p>)</span>
</span></span></code></pre></div><p>Für die Zeitstrahl-Analyse werden die Tweets nun nach Veröffentlichkeitsdatum geordnet und gruppiert:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Prepare data for timeline analysis</span>
</span></span><span class=line><span class=cl><span class=n>tweets_uncoded</span> <span class=o>&lt;-</span> <span class=n>tweets</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=nf>is.na</span><span class=p>(</span><span class=n>code</span><span class=p>))</span> <span class=o>%&gt;%</span> <span class=c1># Filter out coded tweets - the dates of these got messed up while coding unfortunately..</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>datetime</span> <span class=o>=</span> <span class=n>datetime</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>    <span class=nf>parse_date_time</span><span class=p>(</span><span class=n>orders</span> <span class=o>=</span> <span class=s>&#39; %Y-%m-%d %H%M%S&#39;</span><span class=p>)</span> <span class=c1># Parse date</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>datetime</span> <span class=o>=</span> <span class=n>datetime</span> <span class=o>+</span> <span class=m>1</span><span class=o>*</span><span class=m>60</span><span class=o>*</span><span class=m>60</span><span class=p>)</span> <span class=o>%&gt;%</span> <span class=c1># Set time from UTC to CET.</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>datetime</span> <span class=o>=</span> <span class=n>datetime</span> <span class=o>%&gt;%</span> <span class=nf>round</span><span class=p>(</span><span class=n>units</span> <span class=o>=</span> <span class=s>&#39;days&#39;</span><span class=p>)</span> <span class=o>%&gt;%</span> <span class=nf>as.POSIXct</span><span class=p>())</span> <span class=c1># Remove the time, we just need the dates</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># What timerange do we have?</span>
</span></span><span class=line><span class=cl><span class=c1># tweets_uncoded %&gt;% pull(datetime) %&gt;% min()</span>
</span></span><span class=line><span class=cl><span class=c1># tweets_uncoded %&gt;% pull(datetime) %&gt;% max()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Group by day</span>
</span></span><span class=line><span class=cl><span class=n>tweet_groups</span> <span class=o>&lt;-</span> <span class=n>tweets_uncoded</span> <span class=o>%&gt;%</span> <span class=nf>group_by</span><span class=p>(</span><span class=n>datetime</span><span class=p>)</span> <span class=o>%&gt;%</span> <span class=nf>tally</span><span class=p>()</span> <span class=o>%&gt;%</span> <span class=nf>rename</span><span class=p>(</span><span class=n>date</span> <span class=o>=</span> <span class=n>datetime</span><span class=p>)</span>
</span></span></code></pre></div><p>Mit dem R-Package <code>tidycovid19</code> können wir ein Dataset mit der Anzahl COVID-19-Fällen laden und diese ebenfalls nach Datum gruppieren. Da nur Tweets aus dem deutssprachigen Raum analysiert werden, berücksichtigen wir auch nur Fallzahlen aus Deutschland, Österreich und der Schweiz. Dabei werden die COVID-19-Fallzahlen für die Visualisierung proportional zur Anzahl Tweets herunterskaliert - die Messeinheit der Y-Achse stimmt also in der Grafik nur für die Anzahl Tweets, nicht aber für die COVID-19-Fallzahlen!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Get covid-cases via tidycovid19 package</span>
</span></span><span class=line><span class=cl><span class=c1># covid &lt;- download_merged_data(cached = TRUE)</span>
</span></span><span class=line><span class=cl><span class=n>covid</span> <span class=o>&lt;-</span> <span class=n>jsonlite</span><span class=o>::</span><span class=nf>stream_in</span><span class=p>(</span><span class=nf>file</span><span class=p>(</span><span class=s>&#34;covid_data.json&#34;</span><span class=p>),</span> <span class=n>verbose</span> <span class=o>=</span> <span class=bp>F</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># German cases</span>
</span></span><span class=line><span class=cl><span class=n>covidDE</span> <span class=o>&lt;-</span> <span class=n>covid</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>iso3c</span> <span class=o>==</span> <span class=s>&#34;DEU&#34;</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>select</span><span class=p>(</span><span class=n>date</span><span class=p>,</span> <span class=n>confirmed</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>confirmed</span> <span class=o>=</span> <span class=nf>round</span><span class=p>(</span><span class=n>confirmed</span><span class=o>/</span><span class=m>382</span><span class=p>))</span> <span class=o>%&gt;%</span> <span class=c1># Normalize case numbers proportionaly to the maximum numbers of tweets (max (max 400/day)</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>date</span> <span class=o>=</span> <span class=n>date</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>    <span class=c1># Parse date.</span>
</span></span><span class=line><span class=cl>    <span class=nf>parse_date_time</span><span class=p>(</span><span class=n>orders</span> <span class=o>=</span> <span class=s>&#39; %Y-%m-%d&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>date</span> <span class=o>&gt;</span> <span class=s>&#34;2020-02-01&#34;</span> <span class=o>&amp;</span> <span class=n>date</span> <span class=o>&lt;</span><span class=s>&#34;2020-04-25&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Swiss cases</span>
</span></span><span class=line><span class=cl><span class=n>covidCH</span> <span class=o>&lt;-</span> <span class=n>covid</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>iso3c</span> <span class=o>==</span> <span class=s>&#34;CHE&#34;</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>select</span><span class=p>(</span><span class=n>date</span><span class=p>,</span> <span class=n>confirmed</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>confirmed</span> <span class=o>=</span> <span class=nf>round</span><span class=p>(</span><span class=n>confirmed</span><span class=o>/</span><span class=m>71</span><span class=p>))</span> <span class=o>%&gt;%</span> <span class=c1># Normalize case numbers proportionaly to the maximum numbers of tweets (max 400/day)</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>date</span> <span class=o>=</span> <span class=n>date</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>    <span class=c1># Parse date.</span>
</span></span><span class=line><span class=cl>    <span class=nf>parse_date_time</span><span class=p>(</span><span class=n>orders</span> <span class=o>=</span> <span class=s>&#39; %Y-%m-%d&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>date</span> <span class=o>&gt;</span> <span class=s>&#34;2020-02-01&#34;</span> <span class=o>&amp;</span> <span class=n>date</span> <span class=o>&lt;</span><span class=s>&#34;2020-04-25&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Austrian cases</span>
</span></span><span class=line><span class=cl><span class=n>covidAT</span> <span class=o>&lt;-</span> <span class=n>covid</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>iso3c</span> <span class=o>==</span> <span class=s>&#34;AUT&#34;</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>select</span><span class=p>(</span><span class=n>date</span><span class=p>,</span> <span class=n>confirmed</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>confirmed</span> <span class=o>=</span> <span class=nf>round</span><span class=p>(</span><span class=n>confirmed</span><span class=o>/</span><span class=m>37</span><span class=p>))</span> <span class=o>%&gt;%</span> <span class=c1># Normalize case numbers proportionaly to the maximum numbers of tweets (max 400/day)</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>date</span> <span class=o>=</span> <span class=n>date</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>    <span class=c1># Parse date.</span>
</span></span><span class=line><span class=cl>    <span class=nf>parse_date_time</span><span class=p>(</span><span class=n>orders</span> <span class=o>=</span> <span class=s>&#39; %Y-%m-%d&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>date</span> <span class=o>&gt;</span> <span class=s>&#34;2020-02-01&#34;</span> <span class=o>&amp;</span> <span class=n>date</span> <span class=o>&lt;</span><span class=s>&#34;2020-04-25&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># join tweets &amp; covid-cases</span>
</span></span><span class=line><span class=cl><span class=n>tweets_covid</span> <span class=o>&lt;-</span> <span class=n>covidDE</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>  <span class=nf>right_join</span><span class=p>(</span><span class=n>covidCH</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s>&#34;date&#34;</span><span class=p>)</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>  <span class=nf>right_join</span><span class=p>(</span><span class=n>covidAT</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s>&#34;date&#34;</span><span class=p>)</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>  <span class=nf>right_join</span><span class=p>(</span><span class=n>tweet_groups</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s>&#34;date&#34;</span><span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>select</span><span class=p>(</span><span class=n>date</span><span class=p>,</span> <span class=n>Covid_DE</span> <span class=o>=</span> <span class=n>confirmed.x</span><span class=p>,</span> <span class=n>Covid_CH</span> <span class=o>=</span> <span class=n>confirmed.y</span><span class=p>,</span> <span class=n>Covid_AUT</span> <span class=o>=</span> <span class=n>confirmed</span><span class=p>,</span> <span class=n>Num_Tweets</span><span class=o>=</span><span class=n>n</span><span class=p>)</span> <span class=c1># rename columns for plotly</span>
</span></span></code></pre></div><p>Die nach Tag gruppierte Anzahl Tweets und COVID-19-Fälle können nun visualisiert werden:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># &#34;Melt&#34; the dates for plotting</span>
</span></span><span class=line><span class=cl><span class=n>melted</span> <span class=o>&lt;-</span> <span class=nf>melt</span><span class=p>(</span><span class=n>tweets_covid</span><span class=p>,</span> <span class=n>id</span><span class=o>=</span><span class=s>&#34;date&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Prepare plot</span>
</span></span><span class=line><span class=cl><span class=n>plot</span> <span class=o>&lt;-</span> <span class=nf>ggplot</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>melted</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nf>aes</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=n>date</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>value</span><span class=p>,</span> <span class=n>colour</span><span class=o>=</span><span class=n>variable</span><span class=p>))</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>    <span class=nf>geom_line</span><span class=p>()</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>    <span class=nf>labs</span><span class=p>(</span><span class=n>title</span> <span class=o>=</span> <span class=s>&#39;Anzahl QAnon-Tweets vs. COVID-19-Fälle&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>y</span> <span class=o>=</span> <span class=s>&#39;Anzahl Tweets&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>x</span> <span class=o>=</span> <span class=s>&#39;Datum&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>subtitle</span> <span class=o>=</span> <span class=nf>str_c</span><span class=p>(</span><span class=s>&#34;Total 11&#39;363 Tweets aus dem Zeitraum 01.02.2020-24.04.2020&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=s>&#34;(COVID-19-Fälle sind normalisiert, Y-Achse misst nur Anzahl Tweets)&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>           <span class=n>colour</span> <span class=o>=</span> <span class=s>&#34;&#34;</span>
</span></span><span class=line><span class=cl>           <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># plot it</span>
</span></span><span class=line><span class=cl><span class=n>fig</span> <span class=o>&lt;-</span> <span class=nf>ggplotly</span><span class=p>(</span><span class=n>plot</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>fig</span>
</span></span></code></pre></div><div class="plotly html-widget html-fill-item" id=htmlwidget-1 style=width:672px;height:480px></div><script type=application/json data-for=htmlwidget-1>{"x":{"data":[{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,2,2,3,3,4,5,5,10,12,15,19,24,32,40,52,58,65,76,86,98,115,133,151,163,175,188,204,222,239,252,262,271,282,297,309,320,327,335,341,344,353,360,370,375,380,385,388,394,401,406],"text":["date: 2020-02-01<br />value:   0<br />variable: Covid_DE","date: 2020-02-02<br />value:   0<br />variable: Covid_DE","date: 2020-02-03<br />value:   0<br />variable: Covid_DE","date: 2020-02-04<br />value:   0<br />variable: Covid_DE","date: 2020-02-05<br />value:   0<br />variable: Covid_DE","date: 2020-02-06<br />value:   0<br />variable: Covid_DE","date: 2020-02-07<br />value:   0<br />variable: Covid_DE","date: 2020-02-08<br />value:   0<br />variable: Covid_DE","date: 2020-02-09<br />value:   0<br />variable: Covid_DE","date: 2020-02-10<br />value:   0<br />variable: Covid_DE","date: 2020-02-11<br />value:   0<br />variable: Covid_DE","date: 2020-02-12<br />value:   0<br />variable: Covid_DE","date: 2020-02-13<br />value:   0<br />variable: Covid_DE","date: 2020-02-14<br />value:   0<br />variable: Covid_DE","date: 2020-02-15<br />value:   0<br />variable: Covid_DE","date: 2020-02-16<br />value:   0<br />variable: Covid_DE","date: 2020-02-17<br />value:   0<br />variable: Covid_DE","date: 2020-02-18<br />value:   0<br />variable: Covid_DE","date: 2020-02-19<br />value:   0<br />variable: Covid_DE","date: 2020-02-20<br />value:   0<br />variable: Covid_DE","date: 2020-02-21<br />value:   0<br />variable: Covid_DE","date: 2020-02-22<br />value:   0<br />variable: Covid_DE","date: 2020-02-23<br />value:   0<br />variable: Covid_DE","date: 2020-02-24<br />value:   0<br />variable: Covid_DE","date: 2020-02-25<br />value:   0<br />variable: Covid_DE","date: 2020-02-26<br />value:   0<br />variable: Covid_DE","date: 2020-02-27<br />value:   0<br />variable: Covid_DE","date: 2020-02-28<br />value:   0<br />variable: Covid_DE","date: 2020-02-29<br />value:   0<br />variable: Covid_DE","date: 2020-03-01<br />value:   0<br />variable: Covid_DE","date: 2020-03-02<br />value:   0<br />variable: Covid_DE","date: 2020-03-03<br />value:   1<br />variable: Covid_DE","date: 2020-03-04<br />value:   1<br />variable: Covid_DE","date: 2020-03-05<br />value:   1<br />variable: Covid_DE","date: 2020-03-06<br />value:   2<br />variable: Covid_DE","date: 2020-03-07<br />value:   2<br />variable: Covid_DE","date: 2020-03-08<br />value:   3<br />variable: Covid_DE","date: 2020-03-09<br />value:   3<br />variable: Covid_DE","date: 2020-03-10<br />value:   4<br />variable: Covid_DE","date: 2020-03-11<br />value:   5<br />variable: Covid_DE","date: 2020-03-12<br />value:   5<br />variable: Covid_DE","date: 2020-03-13<br />value:  10<br />variable: Covid_DE","date: 2020-03-14<br />value:  12<br />variable: Covid_DE","date: 2020-03-15<br />value:  15<br />variable: Covid_DE","date: 2020-03-16<br />value:  19<br />variable: Covid_DE","date: 2020-03-17<br />value:  24<br />variable: Covid_DE","date: 2020-03-18<br />value:  32<br />variable: Covid_DE","date: 2020-03-19<br />value:  40<br />variable: Covid_DE","date: 2020-03-20<br />value:  52<br />variable: Covid_DE","date: 2020-03-21<br />value:  58<br />variable: Covid_DE","date: 2020-03-22<br />value:  65<br />variable: Covid_DE","date: 2020-03-23<br />value:  76<br />variable: Covid_DE","date: 2020-03-24<br />value:  86<br />variable: Covid_DE","date: 2020-03-25<br />value:  98<br />variable: Covid_DE","date: 2020-03-26<br />value: 115<br />variable: Covid_DE","date: 2020-03-27<br />value: 133<br />variable: Covid_DE","date: 2020-03-28<br />value: 151<br />variable: Covid_DE","date: 2020-03-29<br />value: 163<br />variable: Covid_DE","date: 2020-03-30<br />value: 175<br />variable: Covid_DE","date: 2020-03-31<br />value: 188<br />variable: Covid_DE","date: 2020-04-01<br />value: 204<br />variable: Covid_DE","date: 2020-04-02<br />value: 222<br />variable: Covid_DE","date: 2020-04-03<br />value: 239<br />variable: Covid_DE","date: 2020-04-04<br />value: 252<br />variable: Covid_DE","date: 2020-04-05<br />value: 262<br />variable: Covid_DE","date: 2020-04-06<br />value: 271<br />variable: Covid_DE","date: 2020-04-07<br />value: 282<br />variable: Covid_DE","date: 2020-04-08<br />value: 297<br />variable: Covid_DE","date: 2020-04-09<br />value: 309<br />variable: Covid_DE","date: 2020-04-10<br />value: 320<br />variable: Covid_DE","date: 2020-04-11<br />value: 327<br />variable: Covid_DE","date: 2020-04-12<br />value: 335<br />variable: Covid_DE","date: 2020-04-13<br />value: 341<br />variable: Covid_DE","date: 2020-04-14<br />value: 344<br />variable: Covid_DE","date: 2020-04-15<br />value: 353<br />variable: Covid_DE","date: 2020-04-16<br />value: 360<br />variable: Covid_DE","date: 2020-04-17<br />value: 370<br />variable: Covid_DE","date: 2020-04-18<br />value: 375<br />variable: Covid_DE","date: 2020-04-19<br />value: 380<br />variable: Covid_DE","date: 2020-04-20<br />value: 385<br />variable: Covid_DE","date: 2020-04-21<br />value: 388<br />variable: Covid_DE","date: 2020-04-22<br />value: 394<br />variable: Covid_DE","date: 2020-04-23<br />value: 401<br />variable: Covid_DE","date: 2020-04-24<br />value: 406<br />variable: Covid_DE"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(248,118,109,1)","dash":"solid"},"hoveron":"points","name":"Covid_DE","legendgroup":"Covid_DE","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,2,3,4,5,5,7,9,9,16,19,31,31,38,43,57,75,93,105,124,139,153,166,182,198,209,224,234,250,265,276,289,297,305,313,328,339,346,354,358,362,365,371,377,381,386,391,394,395,398,401,404],"text":["date: 2020-02-01<br />value:   0<br />variable: Covid_CH","date: 2020-02-02<br />value:   0<br />variable: Covid_CH","date: 2020-02-03<br />value:   0<br />variable: Covid_CH","date: 2020-02-04<br />value:   0<br />variable: Covid_CH","date: 2020-02-05<br />value:   0<br />variable: Covid_CH","date: 2020-02-06<br />value:   0<br />variable: Covid_CH","date: 2020-02-07<br />value:   0<br />variable: Covid_CH","date: 2020-02-08<br />value:   0<br />variable: Covid_CH","date: 2020-02-09<br />value:   0<br />variable: Covid_CH","date: 2020-02-10<br />value:   0<br />variable: Covid_CH","date: 2020-02-11<br />value:   0<br />variable: Covid_CH","date: 2020-02-12<br />value:   0<br />variable: Covid_CH","date: 2020-02-13<br />value:   0<br />variable: Covid_CH","date: 2020-02-14<br />value:   0<br />variable: Covid_CH","date: 2020-02-15<br />value:   0<br />variable: Covid_CH","date: 2020-02-16<br />value:   0<br />variable: Covid_CH","date: 2020-02-17<br />value:   0<br />variable: Covid_CH","date: 2020-02-18<br />value:   0<br />variable: Covid_CH","date: 2020-02-19<br />value:   0<br />variable: Covid_CH","date: 2020-02-20<br />value:   0<br />variable: Covid_CH","date: 2020-02-21<br />value:   0<br />variable: Covid_CH","date: 2020-02-22<br />value:   0<br />variable: Covid_CH","date: 2020-02-23<br />value:   0<br />variable: Covid_CH","date: 2020-02-24<br />value:   0<br />variable: Covid_CH","date: 2020-02-25<br />value:   0<br />variable: Covid_CH","date: 2020-02-26<br />value:   0<br />variable: Covid_CH","date: 2020-02-27<br />value:   0<br />variable: Covid_CH","date: 2020-02-28<br />value:   0<br />variable: Covid_CH","date: 2020-02-29<br />value:   0<br />variable: Covid_CH","date: 2020-03-01<br />value:   0<br />variable: Covid_CH","date: 2020-03-02<br />value:   1<br />variable: Covid_CH","date: 2020-03-03<br />value:   1<br />variable: Covid_CH","date: 2020-03-04<br />value:   1<br />variable: Covid_CH","date: 2020-03-05<br />value:   2<br />variable: Covid_CH","date: 2020-03-06<br />value:   3<br />variable: Covid_CH","date: 2020-03-07<br />value:   4<br />variable: Covid_CH","date: 2020-03-08<br />value:   5<br />variable: Covid_CH","date: 2020-03-09<br />value:   5<br />variable: Covid_CH","date: 2020-03-10<br />value:   7<br />variable: Covid_CH","date: 2020-03-11<br />value:   9<br />variable: Covid_CH","date: 2020-03-12<br />value:   9<br />variable: Covid_CH","date: 2020-03-13<br />value:  16<br />variable: Covid_CH","date: 2020-03-14<br />value:  19<br />variable: Covid_CH","date: 2020-03-15<br />value:  31<br />variable: Covid_CH","date: 2020-03-16<br />value:  31<br />variable: Covid_CH","date: 2020-03-17<br />value:  38<br />variable: Covid_CH","date: 2020-03-18<br />value:  43<br />variable: Covid_CH","date: 2020-03-19<br />value:  57<br />variable: Covid_CH","date: 2020-03-20<br />value:  75<br />variable: Covid_CH","date: 2020-03-21<br />value:  93<br />variable: Covid_CH","date: 2020-03-22<br />value: 105<br />variable: Covid_CH","date: 2020-03-23<br />value: 124<br />variable: Covid_CH","date: 2020-03-24<br />value: 139<br />variable: Covid_CH","date: 2020-03-25<br />value: 153<br />variable: Covid_CH","date: 2020-03-26<br />value: 166<br />variable: Covid_CH","date: 2020-03-27<br />value: 182<br />variable: Covid_CH","date: 2020-03-28<br />value: 198<br />variable: Covid_CH","date: 2020-03-29<br />value: 209<br />variable: Covid_CH","date: 2020-03-30<br />value: 224<br />variable: Covid_CH","date: 2020-03-31<br />value: 234<br />variable: Covid_CH","date: 2020-04-01<br />value: 250<br />variable: Covid_CH","date: 2020-04-02<br />value: 265<br />variable: Covid_CH","date: 2020-04-03<br />value: 276<br />variable: Covid_CH","date: 2020-04-04<br />value: 289<br />variable: Covid_CH","date: 2020-04-05<br />value: 297<br />variable: Covid_CH","date: 2020-04-06<br />value: 305<br />variable: Covid_CH","date: 2020-04-07<br />value: 313<br />variable: Covid_CH","date: 2020-04-08<br />value: 328<br />variable: Covid_CH","date: 2020-04-09<br />value: 339<br />variable: Covid_CH","date: 2020-04-10<br />value: 346<br />variable: Covid_CH","date: 2020-04-11<br />value: 354<br />variable: Covid_CH","date: 2020-04-12<br />value: 358<br />variable: Covid_CH","date: 2020-04-13<br />value: 362<br />variable: Covid_CH","date: 2020-04-14<br />value: 365<br />variable: Covid_CH","date: 2020-04-15<br />value: 371<br />variable: Covid_CH","date: 2020-04-16<br />value: 377<br />variable: Covid_CH","date: 2020-04-17<br />value: 381<br />variable: Covid_CH","date: 2020-04-18<br />value: 386<br />variable: Covid_CH","date: 2020-04-19<br />value: 391<br />variable: Covid_CH","date: 2020-04-20<br />value: 394<br />variable: Covid_CH","date: 2020-04-21<br />value: 395<br />variable: Covid_CH","date: 2020-04-22<br />value: 398<br />variable: Covid_CH","date: 2020-04-23<br />value: 401<br />variable: Covid_CH","date: 2020-04-24<br />value: 404<br />variable: Covid_CH"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(124,174,0,1)","dash":"solid"},"hoveron":"points","name":"Covid_CH","legendgroup":"Covid_CH","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,2,3,4,5,7,8,14,18,23,28,36,44,54,65,76,97,121,143,151,187,207,224,238,260,275,289,301,311,318,326,332,342,350,358,366,373,377,379,384,387,391,394,397,399,400,402,403,405,407],"text":["date: 2020-02-01<br />value:   0<br />variable: Covid_AUT","date: 2020-02-02<br />value:   0<br />variable: Covid_AUT","date: 2020-02-03<br />value:   0<br />variable: Covid_AUT","date: 2020-02-04<br />value:   0<br />variable: Covid_AUT","date: 2020-02-05<br />value:   0<br />variable: Covid_AUT","date: 2020-02-06<br />value:   0<br />variable: Covid_AUT","date: 2020-02-07<br />value:   0<br />variable: Covid_AUT","date: 2020-02-08<br />value:   0<br />variable: Covid_AUT","date: 2020-02-09<br />value:   0<br />variable: Covid_AUT","date: 2020-02-10<br />value:   0<br />variable: Covid_AUT","date: 2020-02-11<br />value:   0<br />variable: Covid_AUT","date: 2020-02-12<br />value:   0<br />variable: Covid_AUT","date: 2020-02-13<br />value:   0<br />variable: Covid_AUT","date: 2020-02-14<br />value:   0<br />variable: Covid_AUT","date: 2020-02-15<br />value:   0<br />variable: Covid_AUT","date: 2020-02-16<br />value:   0<br />variable: Covid_AUT","date: 2020-02-17<br />value:   0<br />variable: Covid_AUT","date: 2020-02-18<br />value:   0<br />variable: Covid_AUT","date: 2020-02-19<br />value:   0<br />variable: Covid_AUT","date: 2020-02-20<br />value:   0<br />variable: Covid_AUT","date: 2020-02-21<br />value:   0<br />variable: Covid_AUT","date: 2020-02-22<br />value:   0<br />variable: Covid_AUT","date: 2020-02-23<br />value:   0<br />variable: Covid_AUT","date: 2020-02-24<br />value:   0<br />variable: Covid_AUT","date: 2020-02-25<br />value:   0<br />variable: Covid_AUT","date: 2020-02-26<br />value:   0<br />variable: Covid_AUT","date: 2020-02-27<br />value:   0<br />variable: Covid_AUT","date: 2020-02-28<br />value:   0<br />variable: Covid_AUT","date: 2020-02-29<br />value:   0<br />variable: Covid_AUT","date: 2020-03-01<br />value:   0<br />variable: Covid_AUT","date: 2020-03-02<br />value:   0<br />variable: Covid_AUT","date: 2020-03-03<br />value:   1<br />variable: Covid_AUT","date: 2020-03-04<br />value:   1<br />variable: Covid_AUT","date: 2020-03-05<br />value:   1<br />variable: Covid_AUT","date: 2020-03-06<br />value:   1<br />variable: Covid_AUT","date: 2020-03-07<br />value:   2<br />variable: Covid_AUT","date: 2020-03-08<br />value:   3<br />variable: Covid_AUT","date: 2020-03-09<br />value:   4<br />variable: Covid_AUT","date: 2020-03-10<br />value:   5<br />variable: Covid_AUT","date: 2020-03-11<br />value:   7<br />variable: Covid_AUT","date: 2020-03-12<br />value:   8<br />variable: Covid_AUT","date: 2020-03-13<br />value:  14<br />variable: Covid_AUT","date: 2020-03-14<br />value:  18<br />variable: Covid_AUT","date: 2020-03-15<br />value:  23<br />variable: Covid_AUT","date: 2020-03-16<br />value:  28<br />variable: Covid_AUT","date: 2020-03-17<br />value:  36<br />variable: Covid_AUT","date: 2020-03-18<br />value:  44<br />variable: Covid_AUT","date: 2020-03-19<br />value:  54<br />variable: Covid_AUT","date: 2020-03-20<br />value:  65<br />variable: Covid_AUT","date: 2020-03-21<br />value:  76<br />variable: Covid_AUT","date: 2020-03-22<br />value:  97<br />variable: Covid_AUT","date: 2020-03-23<br />value: 121<br />variable: Covid_AUT","date: 2020-03-24<br />value: 143<br />variable: Covid_AUT","date: 2020-03-25<br />value: 151<br />variable: Covid_AUT","date: 2020-03-26<br />value: 187<br />variable: Covid_AUT","date: 2020-03-27<br />value: 207<br />variable: Covid_AUT","date: 2020-03-28<br />value: 224<br />variable: Covid_AUT","date: 2020-03-29<br />value: 238<br />variable: Covid_AUT","date: 2020-03-30<br />value: 260<br />variable: Covid_AUT","date: 2020-03-31<br />value: 275<br />variable: Covid_AUT","date: 2020-04-01<br />value: 289<br />variable: Covid_AUT","date: 2020-04-02<br />value: 301<br />variable: Covid_AUT","date: 2020-04-03<br />value: 311<br />variable: Covid_AUT","date: 2020-04-04<br />value: 318<br />variable: Covid_AUT","date: 2020-04-05<br />value: 326<br />variable: Covid_AUT","date: 2020-04-06<br />value: 332<br />variable: Covid_AUT","date: 2020-04-07<br />value: 342<br />variable: Covid_AUT","date: 2020-04-08<br />value: 350<br />variable: Covid_AUT","date: 2020-04-09<br />value: 358<br />variable: Covid_AUT","date: 2020-04-10<br />value: 366<br />variable: Covid_AUT","date: 2020-04-11<br />value: 373<br />variable: Covid_AUT","date: 2020-04-12<br />value: 377<br />variable: Covid_AUT","date: 2020-04-13<br />value: 379<br />variable: Covid_AUT","date: 2020-04-14<br />value: 384<br />variable: Covid_AUT","date: 2020-04-15<br />value: 387<br />variable: Covid_AUT","date: 2020-04-16<br />value: 391<br />variable: Covid_AUT","date: 2020-04-17<br />value: 394<br />variable: Covid_AUT","date: 2020-04-18<br />value: 397<br />variable: Covid_AUT","date: 2020-04-19<br />value: 399<br />variable: Covid_AUT","date: 2020-04-20<br />value: 400<br />variable: Covid_AUT","date: 2020-04-21<br />value: 402<br />variable: Covid_AUT","date: 2020-04-22<br />value: 403<br />variable: Covid_AUT","date: 2020-04-23<br />value: 405<br />variable: Covid_AUT","date: 2020-04-24<br />value: 407<br />variable: Covid_AUT"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(0,191,196,1)","dash":"solid"},"hoveron":"points","name":"Covid_AUT","legendgroup":"Covid_AUT","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[10,38,34,29,33,58,40,37,43,47,48,55,46,36,51,36,38,43,63,67,107,72,51,67,41,52,47,41,36,25,48,33,42,37,40,42,54,51,73,56,73,73,56,112,91,117,114,176,158,153,132,160,136,127,128,207,121,118,169,156,136,103,214,196,173,168,178,161,209,201,219,284,309,385,283,290,346,381,287,316,274,199,384,319],"text":["date: 2020-02-01<br />value:  10<br />variable: Num_Tweets","date: 2020-02-02<br />value:  38<br />variable: Num_Tweets","date: 2020-02-03<br />value:  34<br />variable: Num_Tweets","date: 2020-02-04<br />value:  29<br />variable: Num_Tweets","date: 2020-02-05<br />value:  33<br />variable: Num_Tweets","date: 2020-02-06<br />value:  58<br />variable: Num_Tweets","date: 2020-02-07<br />value:  40<br />variable: Num_Tweets","date: 2020-02-08<br />value:  37<br />variable: Num_Tweets","date: 2020-02-09<br />value:  43<br />variable: Num_Tweets","date: 2020-02-10<br />value:  47<br />variable: Num_Tweets","date: 2020-02-11<br />value:  48<br />variable: Num_Tweets","date: 2020-02-12<br />value:  55<br />variable: Num_Tweets","date: 2020-02-13<br />value:  46<br />variable: Num_Tweets","date: 2020-02-14<br />value:  36<br />variable: Num_Tweets","date: 2020-02-15<br />value:  51<br />variable: Num_Tweets","date: 2020-02-16<br />value:  36<br />variable: Num_Tweets","date: 2020-02-17<br />value:  38<br />variable: Num_Tweets","date: 2020-02-18<br />value:  43<br />variable: Num_Tweets","date: 2020-02-19<br />value:  63<br />variable: Num_Tweets","date: 2020-02-20<br />value:  67<br />variable: Num_Tweets","date: 2020-02-21<br />value: 107<br />variable: Num_Tweets","date: 2020-02-22<br />value:  72<br />variable: Num_Tweets","date: 2020-02-23<br />value:  51<br />variable: Num_Tweets","date: 2020-02-24<br />value:  67<br />variable: Num_Tweets","date: 2020-02-25<br />value:  41<br />variable: Num_Tweets","date: 2020-02-26<br />value:  52<br />variable: Num_Tweets","date: 2020-02-27<br />value:  47<br />variable: Num_Tweets","date: 2020-02-28<br />value:  41<br />variable: Num_Tweets","date: 2020-02-29<br />value:  36<br />variable: Num_Tweets","date: 2020-03-01<br />value:  25<br />variable: Num_Tweets","date: 2020-03-02<br />value:  48<br />variable: Num_Tweets","date: 2020-03-03<br />value:  33<br />variable: Num_Tweets","date: 2020-03-04<br />value:  42<br />variable: Num_Tweets","date: 2020-03-05<br />value:  37<br />variable: Num_Tweets","date: 2020-03-06<br />value:  40<br />variable: Num_Tweets","date: 2020-03-07<br />value:  42<br />variable: Num_Tweets","date: 2020-03-08<br />value:  54<br />variable: Num_Tweets","date: 2020-03-09<br />value:  51<br />variable: Num_Tweets","date: 2020-03-10<br />value:  73<br />variable: Num_Tweets","date: 2020-03-11<br />value:  56<br />variable: Num_Tweets","date: 2020-03-12<br />value:  73<br />variable: Num_Tweets","date: 2020-03-13<br />value:  73<br />variable: Num_Tweets","date: 2020-03-14<br />value:  56<br />variable: Num_Tweets","date: 2020-03-15<br />value: 112<br />variable: Num_Tweets","date: 2020-03-16<br />value:  91<br />variable: Num_Tweets","date: 2020-03-17<br />value: 117<br />variable: Num_Tweets","date: 2020-03-18<br />value: 114<br />variable: Num_Tweets","date: 2020-03-19<br />value: 176<br />variable: Num_Tweets","date: 2020-03-20<br />value: 158<br />variable: Num_Tweets","date: 2020-03-21<br />value: 153<br />variable: Num_Tweets","date: 2020-03-22<br />value: 132<br />variable: Num_Tweets","date: 2020-03-23<br />value: 160<br />variable: Num_Tweets","date: 2020-03-24<br />value: 136<br />variable: Num_Tweets","date: 2020-03-25<br />value: 127<br />variable: Num_Tweets","date: 2020-03-26<br />value: 128<br />variable: Num_Tweets","date: 2020-03-27<br />value: 207<br />variable: Num_Tweets","date: 2020-03-28<br />value: 121<br />variable: Num_Tweets","date: 2020-03-29<br />value: 118<br />variable: Num_Tweets","date: 2020-03-30<br />value: 169<br />variable: Num_Tweets","date: 2020-03-31<br />value: 156<br />variable: Num_Tweets","date: 2020-04-01<br />value: 136<br />variable: Num_Tweets","date: 2020-04-02<br />value: 103<br />variable: Num_Tweets","date: 2020-04-03<br />value: 214<br />variable: Num_Tweets","date: 2020-04-04<br />value: 196<br />variable: Num_Tweets","date: 2020-04-05<br />value: 173<br />variable: Num_Tweets","date: 2020-04-06<br />value: 168<br />variable: Num_Tweets","date: 2020-04-07<br />value: 178<br />variable: Num_Tweets","date: 2020-04-08<br />value: 161<br />variable: Num_Tweets","date: 2020-04-09<br />value: 209<br />variable: Num_Tweets","date: 2020-04-10<br />value: 201<br />variable: Num_Tweets","date: 2020-04-11<br />value: 219<br />variable: Num_Tweets","date: 2020-04-12<br />value: 284<br />variable: Num_Tweets","date: 2020-04-13<br />value: 309<br />variable: Num_Tweets","date: 2020-04-14<br />value: 385<br />variable: Num_Tweets","date: 2020-04-15<br />value: 283<br />variable: Num_Tweets","date: 2020-04-16<br />value: 290<br />variable: Num_Tweets","date: 2020-04-17<br />value: 346<br />variable: Num_Tweets","date: 2020-04-18<br />value: 381<br />variable: Num_Tweets","date: 2020-04-19<br />value: 287<br />variable: Num_Tweets","date: 2020-04-20<br />value: 316<br />variable: Num_Tweets","date: 2020-04-21<br />value: 274<br />variable: Num_Tweets","date: 2020-04-22<br />value: 199<br />variable: Num_Tweets","date: 2020-04-23<br />value: 384<br />variable: Num_Tweets","date: 2020-04-24<br />value: 319<br />variable: Num_Tweets"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(199,124,255,1)","dash":"solid"},"hoveron":"points","name":"Num_Tweets","legendgroup":"Num_Tweets","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.762557077625573,"r":7.3059360730593621,"b":40.182648401826498,"l":43.105022831050235},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724},"title":{"text":"Anzahl QAnon-Tweets vs. COVID-19-Fälle","font":{"color":"rgba(0,0,0,1)","family":"","size":17.534246575342465},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[1580156640,1588044960],"tickmode":"array","ticktext":["Feb","Mar","Apr"],"tickvals":[1580515200,1583020800,1585699200],"categoryorder":"array","categoryarray":["Feb","Mar","Apr"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.6529680365296811,"tickwidth":0.66417600664176002,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.68949771689498},"tickangle":0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176002,"zeroline":false,"anchor":"y","title":{"text":"Datum","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-20.350000000000001,427.35000000000002],"tickmode":"array","ticktext":["0","100","200","300","400"],"tickvals":[0,100,200,300,400],"categoryorder":"array","categoryarray":["0","100","200","300","400"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.6529680365296811,"tickwidth":0.66417600664176002,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.68949771689498},"tickangle":0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176002,"zeroline":false,"anchor":"x","title":{"text":"Anzahl Tweets","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","layer":"below","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.8897637795275593,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.68949771689498},"title":{"text":"","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724}}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"fa172f1232b4":{"x":{},"y":{},"colour":{},"type":"scatter"}},"cur_data":"fa172f1232b4","visdat":{"fa172f1232b4":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script><p>Der Plot zeigt deutlich, wie mit steigenden COVID-19-Fallzahlen auch die Anzahl abgesetzter Tweets zur QAnon-Verschwörungstheorie zunahmen. Lässt sich daraus schliessen, dass QAnon Anhänger*innen gewonnen hat? Nicht unbedingt. Die Tweets können sich auch kritisch auf QAnon beziehen und zum Beispiel über die Gefahren der Verschwörungstheorie aufklären wollen.</p><p>In einem weiteren Schritt soll nun durch maschinelles Lernen eine Schätzung versucht werden, wieviele Tweets sich prozentual entweder positiv auf QAnon beziehen und die Verschwörungstheorie verbreiten oder negativ und kritisch auf QAnon hinweisen.</p><h2 id=automatische-inhaltsanalyse-mit-dem-r-package-readme2>Automatische Inhaltsanalyse mit dem R-Package readme2<a hidden class=anchor aria-hidden=true href=#automatische-inhaltsanalyse-mit-dem-r-package-readme2>#</a></h2><p>Für die maschinelle Kategorisierung kommt hier das Package <a href=https://github.com/iqss-research/readme-software>readme2</a> zum Einsatz, welches einen Machine-Learning-Algorithmus für die automatische Inhaltsanalyse von Texten für die Sozialwissenschaften implementiert.</p><p>Wie andere supervised ML-Algorithmen benötigt <code>readme2</code> ein Trainingsset von Daten, mit welchen der Algorithmus trainiert wird. Dieses Trainingsset muss manuell erstellt werden, indem zunächst ein Codierschema festgelegt und dann die Tweets mit einem Code der entsprechenden Kategorie zugeordnet werden. Sobald der Algorithmus trainiert ist, kann damit via Spracherkennung eine Schätzung der Verteilung aller Kategorien in den unkategorisierten Tweets gemacht werden.</p><h3 id=manuelle-codierung-der-qanon-tweets>Manuelle Codierung der QAnon-Tweets<a hidden class=anchor aria-hidden=true href=#manuelle-codierung-der-qanon-tweets>#</a></h3><p>Da ich zur Erstellung des Testsets eine relativ grosse Anzahl Tweets manuell codieren musste (ca. 10% der 11’363 Tweets) und keine Software fand, mit welcher sich das einfach und schnell bewerkstelligen lässt, habe ich dafür eine kleine Web-App entwickelt (<a href=https://github.com/nomaad/tweet_codes>Github-Link</a>). Das Tool lädt zufällig einzelne Tweets aus dem gesamten Datenset, welche sich dann codieren lassen. Der Code wird dem Datensatz als Feld hinzugefügt und einer MongoDB Datenbank gespeichert. Damit wurden 904 Tweets jeweils einer von drei Kategorien zugeordnet (0 = nicht zuordenbar, 1 = kritisch-negativer Bezug, 2 = affirmativ-positiver Bezug). Disclaimer: Ich habe die Tweets in relativ schnellem Tempo alleine ohne Validierung codiert, es können also durchaus Fehlcodierungen vorhanden sein.</p><p><img loading=lazy src=/posts/2020-08-17-categorize-qanon-references-with-machine-learning/tweetcoder.gif></p><h3 id=training-des-readme2-algorithmus>Training des readme2-Algorithmus<a hidden class=anchor aria-hidden=true href=#training-des-readme2-algorithmus>#</a></h3><p>Mit den codierten Tweets lässt sich nun der readme2-Algorithmus trainieren. Zur Textanalyse greift <code>readme2</code> auf word embeddings zurück, welche mit dem <a href=http://text2vec.org/glove.html>GloVe-Algorithmus</a> erstellt wurden. <code>readme2</code> kommt mit englischen Worteinbettungen, ich habe deshalb für diese Analyse zunächst die deutschen Worteinbettungen von <a href=https://deepset.ai/german-word-embeddings>deepset.ai</a> heruntergeladen, welche auf der deutschen Wikipedia trainiert wurden.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Select the tweets which have a code</span>
</span></span><span class=line><span class=cl><span class=n>tweets_coded</span> <span class=o>&lt;-</span> <span class=n>tweets</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=o>!</span><span class=nf>is.na</span><span class=p>(</span><span class=n>code</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Helper function to load word embeddings</span>
</span></span><span class=line><span class=cl><span class=n>loadVecs</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>(</span><span class=n>path</span><span class=p>){</span>
</span></span><span class=line><span class=cl>  <span class=n>wordVecs_corpus</span> <span class=o>&lt;-</span> <span class=n>data.table</span><span class=o>::</span><span class=nf>fread</span><span class=p>(</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>wordVecs_keys</span> <span class=o>&lt;-</span> <span class=n>wordVecs_corpus[[1]]</span><span class=c1>## first row is the name of the term</span>
</span></span><span class=line><span class=cl>  <span class=n>wordVecs_corpus</span> <span class=o>&lt;-</span> <span class=nf>as.matrix </span><span class=p>(</span>  <span class=n>wordVecs_corpus[</span><span class=p>,</span><span class=m>-1</span><span class=n>]</span> <span class=p>)</span>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=nf>row.names</span><span class=p>(</span><span class=n>wordVecs_corpus</span><span class=p>)</span> <span class=o>&lt;-</span> <span class=n>wordVecs_keys</span>
</span></span><span class=line><span class=cl>  <span class=n>wordVecs</span> <span class=o>&lt;-</span> <span class=n>wordVecs_corpus</span>
</span></span><span class=line><span class=cl>  <span class=nf>rm</span><span class=p>(</span><span class=n>wordVecs_corpus</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>rm</span><span class=p>(</span><span class=n>wordVecs_keys</span><span class=p>)</span> <span class=c1>## Remove the original loaded table to save space</span>
</span></span><span class=line><span class=cl>  <span class=nf>saveRDS</span><span class=p>(</span><span class=n>wordVecs</span><span class=p>,</span> <span class=n>file</span> <span class=o>=</span> <span class=s>&#34;wordVecs.rds&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kr>return</span><span class=p>(</span><span class=n>wordVecs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Generate a word vector summary for each document</span>
</span></span><span class=line><span class=cl><span class=c1># Use the german Wikipedia-trained GloVe word embeddings from https://deepset.ai/german-word-embeddings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load word embeddings.. </span>
</span></span><span class=line><span class=cl><span class=c1># wordVecs &lt;-loadVecs(&#39;deepset.ai.german.wikipedia.glove.txt&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># wordVec_summaries = undergrad(documentText = cleanme(tweets_coded$text), wordVecs = wordVecs)</span>
</span></span><span class=line><span class=cl><span class=c1>#saveRDS(wordVec_summaries, file = &#34;wordVec_summaries.rds&#34;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ..or load from cache instead</span>
</span></span><span class=line><span class=cl><span class=n>wordVec_summaries</span> <span class=o>&lt;-</span> <span class=nf>readRDS</span><span class=p>(</span><span class=n>file</span> <span class=o>=</span> <span class=s>&#34;wordVec_summaries.rds&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Mit den deutschen Vektoren können 74% der Wörter in den 904 Tweets zugeordnet werden. Das scheint mir eher wenig, hat aber wohl damit zu tun, dass in den Tweets sehr viele themenspezifische Hashtags zu finden sind. Nun folgt 1) das eigentliche Training, 2) die automatisierte Kategorisierung der Testdaten und 3) der Abgleich zur Überprüfung mit den manuellen Codes der Testdaten. Schritt 1 und 2 werden direkt von der <code>readme()</code>-Funktion implementiert. Davor werden die 904 Tweets per Zufall in ein Test- und ein Trainingsset aufgeteilt. Ich mache drei Durchläufe mit jeweils unterschiedlichen Test- und Trainingssets, um einen besseren Eindruck über die Zuverlässigkeit der Schätzungen zu bekommen.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Evaluate 3 times, how accurate the estimations are with the coded trainingset</span>
</span></span><span class=line><span class=cl><span class=nf>use_virtualenv</span><span class=p>(</span><span class=s>&#34;r-tensorflow&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nf>foreach</span><span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=m>1</span><span class=o>:</span><span class=m>3</span><span class=p>)</span> <span class=o>%do%</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nf>set.seed</span><span class=p>(</span><span class=n>i</span><span class=o>*</span><span class=m>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># 1. Split coded tweets into a test and a training set</span>
</span></span><span class=line><span class=cl>  <span class=n>rnd_train</span> <span class=o>&lt;-</span> <span class=nf>sample</span><span class=p>(</span><span class=nf>c</span><span class=p>(</span><span class=m>0</span><span class=p>,</span><span class=m>1</span><span class=p>),</span> <span class=nf>nrow</span><span class=p>(</span><span class=n>tweets_coded</span><span class=p>),</span> <span class=n>replace</span> <span class=o>=</span> <span class=bp>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>tweets_coded</span><span class=o>$</span><span class=n>trainingset</span> <span class=o>&lt;-</span> <span class=nf>c</span><span class=p>(</span><span class=n>rnd_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1>#length(which(tweets_coded$trainingset == 1))</span>
</span></span><span class=line><span class=cl>  <span class=c1>#length(which(tweets_coded$trainingset == 0))</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># 2. Call readme to make an estimation</span>
</span></span><span class=line><span class=cl>  <span class=n>readme.estimates</span> <span class=o>&lt;-</span> <span class=nf>readme</span><span class=p>(</span><span class=n>dfm</span> <span class=o>=</span> <span class=n>wordVec_summaries</span> <span class=p>,</span> <span class=n>labeledIndicator</span> <span class=o>=</span> <span class=n>tweets_coded</span><span class=o>$</span><span class=n>trainingset</span><span class=p>,</span> <span class=n>categoryVec</span> <span class=o>=</span> <span class=n>tweets_coded</span><span class=o>$</span><span class=n>code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># 3. Compare estimates &amp; actual values</span>
</span></span><span class=line><span class=cl>  <span class=c1># Output proportions estimate</span>
</span></span><span class=line><span class=cl>  <span class=n>estimate</span> <span class=o>&lt;-</span> <span class=n>readme.estimates</span><span class=o>$</span><span class=n>point_readme</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=n>actual</span> <span class=o>&lt;-</span> <span class=nf>table</span><span class=p>(</span><span class=n>tweets_coded</span><span class=o>$</span><span class=n>code[tweets_coded</span><span class=o>$</span><span class=n>trainingset</span> <span class=o>==</span> <span class=m>0</span><span class=n>]</span><span class=p>)</span><span class=o>/</span><span class=nf>sum</span><span class=p>(</span><span class=nf>table</span><span class=p>((</span><span class=n>tweets_coded</span><span class=o>$</span><span class=n>code[tweets_coded</span><span class=o>$</span><span class=n>trainingset</span> <span class=o>==</span> <span class=m>0</span><span class=n>]</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>   
</span></span><span class=line><span class=cl>  <span class=c1># Calculate deviation of estimation from actual value in percent points</span>
</span></span><span class=line><span class=cl>  <span class=n>percentages</span> <span class=o>&lt;-</span> <span class=p>((</span><span class=n>actual</span> <span class=o>-</span> <span class=n>estimate</span><span class=p>)</span><span class=o>/</span><span class=n>actual</span><span class=p>)</span> <span class=o>*</span> <span class=m>100</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><pre><code>## TensorFlow v2.16.2 (~/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow)
## Python v3.10 (~/.virtualenvs/r-tensorflow/bin/python)
## [1] &quot;Performance warning: Rebuilding tensorflow graph...&quot;
## [1] &quot;Building master readme graph...&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.02 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.02 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## TensorFlow v2.16.2 (~/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow)
## Python v3.10 (~/.virtualenvs/r-tensorflow/bin/python)
## [1] &quot;Done with this round of training in 0.02 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## TensorFlow v2.16.2 (~/.virtualenvs/r-tensorflow/lib/python3.10/site-packages/tensorflow)
## Python v3.10 (~/.virtualenvs/r-tensorflow/bin/python)
## [1] &quot;Done with this round of training in 0.02 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.02 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.02 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;
## [1] &quot;Done with this round of training in 0.01 minutes!&quot;

## [[1]]
## 
##          0          1          2 
##  -5.994926 -52.033951  10.389316 
## 
## [[2]]
## 
##          0          1          2 
##  -1.591461 -40.736835   7.467267 
## 
## [[3]]
## 
##          0          1          2 
##  -1.358362 -30.341873   5.887147
</code></pre><p>Geschätzte prozentuale Anteile:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=n>estimate</span>
</span></span></code></pre></div><pre><code>##         0         1         2 
## 0.2352962 0.1512897 0.6134141
</code></pre><p>Tatsächliche prozentuale Anteile:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=n>actual</span>
</span></span></code></pre></div><pre><code>## 
##         0         1         2 
## 0.2321429 0.1160714 0.6517857
</code></pre><p>Abweichung der Schätzung vom tatsächlichen Anteil in Prozent:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=n>percentages</span>
</span></span></code></pre></div><pre><code>## 
##          0          1          2 
##  -1.358362 -30.341873   5.887147
</code></pre><p>Es zeigt sich nach drei Durchgängen, dass mit unseren Trainingsdaten die zweite Kategorie (QAnon verbreitend/affirmativ) unterschätzt, die erste Kategorie (QAnon kritisierend/negativ) jedoch überschätzt wird. Auch die Kategorie null (nicht zuordenbar) wird tendenziell unterschätzt. Für die Interpretation ist dies bei der automatischen Kategorisierung der gesamten 11’363 Tweets zu berücksichtigen.</p><h3 id=automatisierte-schätzung-der-tweet-kategorien>Automatisierte Schätzung der Tweet-Kategorien<a hidden class=anchor aria-hidden=true href=#automatisierte-schätzung-der-tweet-kategorien>#</a></h3><p>Nach diesem Training kann nun eine Schätzung über sämtliche Tweets versucht werden. Dazu muss erst sämtlicher Text gegen die GloVe-Embeddings analysiert werden.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Apply to whole dataset</span>
</span></span><span class=line><span class=cl><span class=n>tweets_all</span> <span class=o>&lt;-</span> <span class=n>tweets</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>trainingset</span> <span class=o>=</span> <span class=nf>ifelse</span><span class=p>(</span><span class=o>!</span><span class=nf>is.na</span><span class=p>(</span><span class=n>code</span><span class=p>),</span> <span class=m>1</span><span class=p>,</span> <span class=nf>ifelse</span><span class=p>(</span><span class=nf>is.na</span><span class=p>(</span><span class=n>code</span><span class=p>),</span> <span class=m>0</span><span class=p>,</span> <span class=kc>NA</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Auskommentiert, da unten von Cache gelesend wird</span>
</span></span><span class=line><span class=cl><span class=c1>#wordVec_summaries_all = undergrad(documentText = cleanme(tweets_all$text), wordVecs = wordVecs)</span>
</span></span></code></pre></div><p>Die unkategorisierten Tweets werden nun in einem Loop für jeden Tag im gesamten Zeitraum aufgesplittet. Für jeden Tag wird dann <code>readme()</code> aufgerufen. Schlussendlich kann so visualisiert werden, ob und wie sich die Anteile der einzelnen Kategorien mit fortschreitendem zeitlichen Verlauf verändert haben. Da die Berechnung auf meinem Rechner mehr als 3 Stunden in Anspruch genommen hat, habe ich die Resultate in eine JSON-Datei gespeichert und lese für dieses RMarkdown nur die exportierte Datei, statt nochmals alles durchrechnen zu lassen.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Parse datetime</span>
</span></span><span class=line><span class=cl><span class=n>tweets_date_sorted</span> <span class=o>&lt;-</span> <span class=n>tweets_all</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>datetime</span> <span class=o>=</span> <span class=n>datetime</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>      <span class=nf>parse_date_time</span><span class=p>(</span><span class=n>orders</span> <span class=o>=</span> <span class=s>&#39; %Y-%m-%d %H%M%S&#39;</span><span class=p>)</span> <span class=c1># Parse date.</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>datetime</span> <span class=o>=</span> <span class=n>datetime</span> <span class=o>+</span> <span class=m>1</span><span class=o>*</span><span class=m>60</span><span class=o>*</span><span class=m>60</span><span class=p>)</span> <span class=o>%&gt;%</span> <span class=c1># Set time from UTC to CET.</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=o>!</span><span class=nf>is.na</span><span class=p>(</span><span class=n>datetime</span><span class=p>))</span> <span class=o>%&gt;%</span><span class=c1># Remove messed up dates..</span>
</span></span><span class=line><span class=cl>  <span class=nf>mutate</span><span class=p>(</span><span class=n>datetime</span> <span class=o>=</span> <span class=n>datetime</span> <span class=o>%&gt;%</span> <span class=nf>round</span><span class=p>(</span><span class=n>units</span> <span class=o>=</span> <span class=s>&#39;days&#39;</span><span class=p>)</span> <span class=o>%&gt;%</span> <span class=nf>as.POSIXct</span><span class=p>())</span> <span class=c1># Remove the time</span>
</span></span></code></pre></div><pre><code>## Warning: There was 1 warning in `mutate()`.
## ℹ In argument: `datetime = datetime %&gt;% parse_date_time(orders = &quot; %Y-%m-%d
##   %H%M%S&quot;)`.
## Caused by warning:
## !  412 failed to parse.
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># Update coded tweets with trainingset-field</span>
</span></span><span class=line><span class=cl><span class=n>tweets_coded</span> <span class=o>&lt;-</span> <span class=n>tweets_all</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>filter</span><span class=p>(</span><span class=n>trainingset</span> <span class=o>==</span> <span class=m>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Group by day</span>
</span></span><span class=line><span class=cl><span class=n>tweet_groups</span> <span class=o>&lt;-</span> <span class=n>tweets_date_sorted</span> <span class=o>%&gt;%</span> <span class=nf>group_by</span><span class=p>(</span><span class=n>datetime</span><span class=p>)</span> <span class=o>%&gt;%</span> <span class=nf>tally</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># We will do estimates per day, therefore let&#39;s encapsulate the estimation in a function</span>
</span></span><span class=line><span class=cl><span class=n>estimate_categories</span> <span class=o>&lt;-</span> <span class=kr>function</span><span class=p>(</span><span class=n>i</span><span class=p>){</span>
</span></span><span class=line><span class=cl>  <span class=c1># Get the tweets for this date</span>
</span></span><span class=line><span class=cl>  <span class=n>tweets_daily</span> <span class=o>&lt;-</span> <span class=n>tweets_date_sorted</span> <span class=o>%&gt;%</span>
</span></span><span class=line><span class=cl>    <span class=nf>filter</span><span class=p>(</span><span class=n>datetime</span> <span class=o>==</span> <span class=n>tweet_groups[i</span><span class=p>,</span><span class=n>]</span><span class=o>$</span><span class=n>datetime</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Merge the daily tweets with the coded tweets from all dates - we need these for training..</span>
</span></span><span class=line><span class=cl>  <span class=n>tweet_set</span> <span class=o>&lt;-</span> <span class=nf>merge</span><span class=p>(</span><span class=n>tweets_daily</span><span class=p>,</span> <span class=n>tweets_coded</span><span class=p>,</span> <span class=n>all</span><span class=o>=</span><span class=kc>TRUE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Calculate the wordvecs for the tweets for this date</span>
</span></span><span class=line><span class=cl>  <span class=n>wordVec_summaries_set</span> <span class=o>=</span> <span class=nf>undergrad</span><span class=p>(</span><span class=n>documentText</span> <span class=o>=</span> <span class=nf>cleanme</span><span class=p>(</span><span class=n>tweet_set</span><span class=o>$</span><span class=n>text</span><span class=p>),</span> <span class=n>wordVecs</span> <span class=o>=</span> <span class=n>wordVecs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Make the estimation</span>
</span></span><span class=line><span class=cl>  <span class=n>estimate</span> <span class=o>&lt;-</span> <span class=nf>readme</span><span class=p>(</span><span class=n>dfm</span> <span class=o>=</span> <span class=n>wordVec_summaries_set</span> <span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>labeledIndicator</span> <span class=o>=</span> <span class=n>tweet_set</span><span class=o>$</span><span class=n>trainingset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>categoryVec</span> <span class=o>=</span> <span class=n>tweet_set</span><span class=o>$</span><span class=n>code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>verbose</span> <span class=o>=</span> <span class=bp>T</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>diagnostics</span> <span class=o>=</span> <span class=bp>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Return result</span>
</span></span><span class=line><span class=cl>  <span class=kr>return</span><span class=p>(</span><span class=n>estimate</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a container for the results per day</span>
</span></span><span class=line><span class=cl><span class=n>estimates</span> <span class=o>&lt;-</span> <span class=nf>data.frame</span><span class=p>(</span><span class=n>date</span> <span class=o>=</span> <span class=n>tweet_groups[1</span><span class=p>,</span><span class=n>]</span><span class=o>$</span><span class=n>datetime</span><span class=p>,</span> <span class=n>estimate</span> <span class=o>=</span> <span class=n>readme.estimates</span><span class=o>$</span><span class=n>point_readme[1]</span><span class=p>,</span> <span class=n>code</span> <span class=o>=</span> <span class=m>1</span><span class=p>,</span> <span class=n>stringsAsFactors</span> <span class=o>=</span> <span class=bp>F</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Caching the results - The algorithm takes about 3-4 hours, I will not let it run again for the RMarkdown file..</span>
</span></span><span class=line><span class=cl><span class=n>estimate_file</span> <span class=o>&lt;-</span> <span class=s>&#34;qanon_readme_estimates_2020-02-01_2020_04_24.json&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kr>if</span><span class=p>(</span><span class=o>!</span><span class=nf>file.exists</span><span class=p>(</span><span class=n>estimate_file</span><span class=p>)){</span>
</span></span><span class=line><span class=cl>  <span class=c1># Calculate estimates. careful: takes ages! (+3h)</span>
</span></span><span class=line><span class=cl>  <span class=nf>foreach</span><span class=p>(</span><span class=n>i</span><span class=o>=</span><span class=m>2</span><span class=o>:</span><span class=nf>nrow</span><span class=p>(</span><span class=n>tweet_groups</span><span class=p>))</span> <span class=o>%do%</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1># Call the function</span>
</span></span><span class=line><span class=cl>    <span class=n>estimate</span> <span class=o>&lt;-</span> <span class=nf>estimate_categories</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Add results to container</span>
</span></span><span class=line><span class=cl>    <span class=n>estimates</span> <span class=o>&lt;-</span> <span class=n>estimates</span> <span class=o>%&gt;%</span> <span class=nf>add_row</span><span class=p>(</span><span class=n>date</span> <span class=o>=</span> <span class=n>tweet_groups[i</span><span class=p>,</span><span class=n>]</span><span class=o>$</span><span class=n>datetime</span><span class=p>,</span> <span class=n>estimate</span> <span class=o>=</span> <span class=n>estimate</span><span class=o>$</span><span class=n>point_readme[1]</span><span class=p>,</span> <span class=n>code</span> <span class=o>=</span> <span class=m>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>estimates</span> <span class=o>&lt;-</span> <span class=n>estimates</span> <span class=o>%&gt;%</span> <span class=nf>add_row</span><span class=p>(</span><span class=n>date</span> <span class=o>=</span> <span class=n>tweet_groups[i</span><span class=p>,</span><span class=n>]</span><span class=o>$</span><span class=n>datetime</span><span class=p>,</span> <span class=n>estimate</span> <span class=o>=</span> <span class=n>estimate</span><span class=o>$</span><span class=n>point_readme[2]</span><span class=p>,</span> <span class=n>code</span> <span class=o>=</span> <span class=m>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>estimates</span> <span class=o>&lt;-</span> <span class=n>estimates</span> <span class=o>%&gt;%</span> <span class=nf>add_row</span><span class=p>(</span><span class=n>date</span> <span class=o>=</span> <span class=n>tweet_groups[i</span><span class=p>,</span><span class=n>]</span><span class=o>$</span><span class=n>datetime</span><span class=p>,</span> <span class=n>estimate</span> <span class=o>=</span> <span class=n>estimate</span><span class=o>$</span><span class=n>point_readme[3]</span><span class=p>,</span> <span class=n>code</span> <span class=o>=</span> <span class=m>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Remove the first record that was created just for the initialization</span>
</span></span><span class=line><span class=cl>  <span class=n>estimates</span> <span class=o>&lt;-</span> <span class=n>estimates[</span><span class=m>-1</span><span class=p>,</span><span class=n>]</span>
</span></span><span class=line><span class=cl>  <span class=n>plotable</span> <span class=o>&lt;-</span> <span class=n>estimates</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>    <span class=nf>pivot_wider</span><span class=p>(</span><span class=n>names_from</span> <span class=o>=</span> <span class=n>code</span><span class=p>,</span> <span class=n>values_from</span> <span class=o>=</span> <span class=n>estimate</span><span class=p>)</span> <span class=c1># Use pivot_wider from tidyr-Package to make data tidy</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Export results as JSON file</span>
</span></span><span class=line><span class=cl>  <span class=n>jsonlite</span><span class=o>::</span><span class=nf>stream_out</span><span class=p>(</span><span class=n>plotable</span><span class=p>,</span> <span class=nf>file</span><span class=p>(</span><span class=n>estimate_file</span><span class=p>),</span> <span class=n>verbose</span> <span class=o>=</span> <span class=bp>F</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=kr>else</span><span class=p>{</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># If file exists, load from file</span>
</span></span><span class=line><span class=cl>  <span class=n>plotable</span> <span class=o>&lt;-</span> <span class=n>jsonlite</span><span class=o>::</span><span class=nf>stream_in</span><span class=p>(</span><span class=nf>file</span><span class=p>(</span><span class=s>&#34;qanon_readme_estimates_2020-02-01_2020_04_24.json&#34;</span><span class=p>),</span> <span class=n>verbose</span> <span class=o>=</span> <span class=bp>F</span><span class=p>)</span>  
</span></span><span class=line><span class=cl>  <span class=n>plotable</span> <span class=o>&lt;-</span> <span class=n>plotable</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>    <span class=nf>mutate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>date</span> <span class=o>=</span> <span class=n>date</span> <span class=o>%&gt;%</span> 
</span></span><span class=line><span class=cl>        <span class=c1># Parse date.</span>
</span></span><span class=line><span class=cl>        <span class=nf>parse_date_time</span><span class=p>(</span><span class=n>orders</span> <span class=o>=</span> <span class=s>&#39; %Y-%m-%d&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plotable</span> <span class=o>%&lt;&gt;%</span>
</span></span><span class=line><span class=cl>  <span class=nf>select</span><span class=p>(</span><span class=n>Datum</span> <span class=o>=</span> <span class=s>&#34;date&#34;</span><span class=p>,</span> <span class=n>N_A</span> <span class=o>=</span> <span class=s>&#34;0&#34;</span><span class=p>,</span> <span class=n>Kritisch</span> <span class=o>=</span> <span class=s>&#34;1&#34;</span><span class=p>,</span> <span class=n>Affirmativ</span> <span class=o>=</span> <span class=s>&#34;2&#34;</span><span class=p>)</span> <span class=c1># rename columns</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># &#34;Melt&#34; the dates..</span>
</span></span><span class=line><span class=cl><span class=n>long</span> <span class=o>&lt;-</span> <span class=nf>melt</span><span class=p>(</span><span class=n>plotable</span><span class=p>,</span> <span class=n>id</span><span class=o>=</span><span class=s>&#34;Datum&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># prepare plot</span>
</span></span><span class=line><span class=cl><span class=n>plot</span> <span class=o>&lt;-</span> <span class=nf>ggplot</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nf>aes</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=n>Datum</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>value</span><span class=p>,</span> <span class=n>colour</span><span class=o>=</span><span class=n>variable</span><span class=p>))</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>  <span class=nf>geom_line</span><span class=p>()</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>  <span class=nf>labs</span><span class=p>(</span><span class=n>title</span> <span class=o>=</span> <span class=s>&#39;Automatische Kategorisierung von QAnon-Tweets&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>y</span> <span class=o>=</span> <span class=s>&#39;Prozentualer Anteil&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>x</span> <span class=o>=</span> <span class=s>&#39;Datum&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>subtitle</span> <span class=o>=</span> <span class=nf>str_c</span><span class=p>(</span><span class=s>&#34;Total 11&#39;363 Tweets aus dem Zeitraum 01.02.2020-24.04.2020,&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=s>&#34;Schätzung durch readme2-Algorithmus&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>       <span class=n>colour</span> <span class=o>=</span> <span class=s>&#34;Kategorien&#34;</span>
</span></span><span class=line><span class=cl>       <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#plot</span>
</span></span><span class=line><span class=cl><span class=n>fig</span> <span class=o>&lt;-</span> <span class=nf>ggplotly</span><span class=p>(</span><span class=n>plot</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>fig</span>
</span></span></code></pre></div><div class="plotly html-widget html-fill-item" id=htmlwidget-2 style=width:672px;height:480px></div><script type=application/json data-for=htmlwidget-2>{"x":{"data":[{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[0.18171000000000001,0.16028999999999999,0.15687999999999999,0.22983000000000001,0.23880000000000001,0.16386999999999999,0.042639999999999997,0.21901999999999999,0.18398999999999999,0.2203,0.13868,0.14074,0.20433999999999999,0.27722000000000002,0.32406000000000001,0.19538,0.35500999999999999,0.17041000000000001,0.47221999999999997,0.30586999999999998,0.23352999999999999,0.24199000000000001,0.14299000000000001,0.39023000000000002,0.31341999999999998,0.26028000000000001,0.29819000000000001,0.23494000000000001,0.22899,0.23122000000000001,0.24987000000000001,0.30623,0.25417000000000001,0.23174,0.28700999999999999,0.2591,0.36042000000000002,0.29992999999999997,0.27743000000000001,0.33101999999999998,0.32238,0.27739000000000003,0.25851000000000002,0.30074000000000001,0.32963999999999999,0.27134000000000003,0.21140999999999999,0.29071999999999998,0.23577999999999999,0.24920999999999999,0.19120999999999999,0.2767,0.26383000000000001,0.28219,0.24002999999999999,0.26834999999999998,0.22575000000000001,0.25625999999999999,0.2944,0.26556000000000002,0.24537,0.24309,0.24424999999999999,0.25656000000000001,0.23938999999999999,0.24041000000000001,0.23257,0.20352999999999999,0.23594000000000001,0.20524000000000001,0.2185,0.22756999999999999,0.25195000000000001,0.27959000000000001,0.26790000000000003,0.25198999999999999,0.30492000000000002,0.35471000000000003,0.31931999999999999,0.30034,0.36936000000000002,0.30282999999999999,0.40299000000000001,0.40938999999999998],"text":["Datum: 2020-02-01<br />value: 0.18171<br />variable: N_A","Datum: 2020-02-02<br />value: 0.16029<br />variable: N_A","Datum: 2020-02-03<br />value: 0.15688<br />variable: N_A","Datum: 2020-02-04<br />value: 0.22983<br />variable: N_A","Datum: 2020-02-05<br />value: 0.23880<br />variable: N_A","Datum: 2020-02-06<br />value: 0.16387<br />variable: N_A","Datum: 2020-02-07<br />value: 0.04264<br />variable: N_A","Datum: 2020-02-08<br />value: 0.21902<br />variable: N_A","Datum: 2020-02-09<br />value: 0.18399<br />variable: N_A","Datum: 2020-02-10<br />value: 0.22030<br />variable: N_A","Datum: 2020-02-11<br />value: 0.13868<br />variable: N_A","Datum: 2020-02-12<br />value: 0.14074<br />variable: N_A","Datum: 2020-02-13<br />value: 0.20434<br />variable: N_A","Datum: 2020-02-14<br />value: 0.27722<br />variable: N_A","Datum: 2020-02-15<br />value: 0.32406<br />variable: N_A","Datum: 2020-02-16<br />value: 0.19538<br />variable: N_A","Datum: 2020-02-17<br />value: 0.35501<br />variable: N_A","Datum: 2020-02-18<br />value: 0.17041<br />variable: N_A","Datum: 2020-02-19<br />value: 0.47222<br />variable: N_A","Datum: 2020-02-20<br />value: 0.30587<br />variable: N_A","Datum: 2020-02-21<br />value: 0.23353<br />variable: N_A","Datum: 2020-02-22<br />value: 0.24199<br />variable: N_A","Datum: 2020-02-23<br />value: 0.14299<br />variable: N_A","Datum: 2020-02-24<br />value: 0.39023<br />variable: N_A","Datum: 2020-02-25<br />value: 0.31342<br />variable: N_A","Datum: 2020-02-26<br />value: 0.26028<br />variable: N_A","Datum: 2020-02-27<br />value: 0.29819<br />variable: N_A","Datum: 2020-02-28<br />value: 0.23494<br />variable: N_A","Datum: 2020-02-29<br />value: 0.22899<br />variable: N_A","Datum: 2020-03-01<br />value: 0.23122<br />variable: N_A","Datum: 2020-03-02<br />value: 0.24987<br />variable: N_A","Datum: 2020-03-03<br />value: 0.30623<br />variable: N_A","Datum: 2020-03-04<br />value: 0.25417<br />variable: N_A","Datum: 2020-03-05<br />value: 0.23174<br />variable: N_A","Datum: 2020-03-06<br />value: 0.28701<br />variable: N_A","Datum: 2020-03-07<br />value: 0.25910<br />variable: N_A","Datum: 2020-03-08<br />value: 0.36042<br />variable: N_A","Datum: 2020-03-09<br />value: 0.29993<br />variable: N_A","Datum: 2020-03-10<br />value: 0.27743<br />variable: N_A","Datum: 2020-03-11<br />value: 0.33102<br />variable: N_A","Datum: 2020-03-12<br />value: 0.32238<br />variable: N_A","Datum: 2020-03-13<br />value: 0.27739<br />variable: N_A","Datum: 2020-03-14<br />value: 0.25851<br />variable: N_A","Datum: 2020-03-15<br />value: 0.30074<br />variable: N_A","Datum: 2020-03-16<br />value: 0.32964<br />variable: N_A","Datum: 2020-03-17<br />value: 0.27134<br />variable: N_A","Datum: 2020-03-18<br />value: 0.21141<br />variable: N_A","Datum: 2020-03-19<br />value: 0.29072<br />variable: N_A","Datum: 2020-03-20<br />value: 0.23578<br />variable: N_A","Datum: 2020-03-21<br />value: 0.24921<br />variable: N_A","Datum: 2020-03-22<br />value: 0.19121<br />variable: N_A","Datum: 2020-03-23<br />value: 0.27670<br />variable: N_A","Datum: 2020-03-24<br />value: 0.26383<br />variable: N_A","Datum: 2020-03-25<br />value: 0.28219<br />variable: N_A","Datum: 2020-03-26<br />value: 0.24003<br />variable: N_A","Datum: 2020-03-27<br />value: 0.26835<br />variable: N_A","Datum: 2020-03-28<br />value: 0.22575<br />variable: N_A","Datum: 2020-03-29<br />value: 0.25626<br />variable: N_A","Datum: 2020-03-30<br />value: 0.29440<br />variable: N_A","Datum: 2020-03-31<br />value: 0.26556<br />variable: N_A","Datum: 2020-04-01<br />value: 0.24537<br />variable: N_A","Datum: 2020-04-02<br />value: 0.24309<br />variable: N_A","Datum: 2020-04-03<br />value: 0.24425<br />variable: N_A","Datum: 2020-04-04<br />value: 0.25656<br />variable: N_A","Datum: 2020-04-05<br />value: 0.23939<br />variable: N_A","Datum: 2020-04-06<br />value: 0.24041<br />variable: N_A","Datum: 2020-04-07<br />value: 0.23257<br />variable: N_A","Datum: 2020-04-08<br />value: 0.20353<br />variable: N_A","Datum: 2020-04-09<br />value: 0.23594<br />variable: N_A","Datum: 2020-04-10<br />value: 0.20524<br />variable: N_A","Datum: 2020-04-11<br />value: 0.21850<br />variable: N_A","Datum: 2020-04-12<br />value: 0.22757<br />variable: N_A","Datum: 2020-04-13<br />value: 0.25195<br />variable: N_A","Datum: 2020-04-14<br />value: 0.27959<br />variable: N_A","Datum: 2020-04-15<br />value: 0.26790<br />variable: N_A","Datum: 2020-04-16<br />value: 0.25199<br />variable: N_A","Datum: 2020-04-17<br />value: 0.30492<br />variable: N_A","Datum: 2020-04-18<br />value: 0.35471<br />variable: N_A","Datum: 2020-04-19<br />value: 0.31932<br />variable: N_A","Datum: 2020-04-20<br />value: 0.30034<br />variable: N_A","Datum: 2020-04-21<br />value: 0.36936<br />variable: N_A","Datum: 2020-04-22<br />value: 0.30283<br />variable: N_A","Datum: 2020-04-23<br />value: 0.40299<br />variable: N_A","Datum: 2020-04-24<br />value: 0.40939<br />variable: N_A"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(248,118,109,1)","dash":"solid"},"hoveron":"points","name":"N_A","legendgroup":"N_A","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[0.01485,0.16394,0.078229999999999994,0.054089999999999999,0.038260000000000002,0.072069999999999995,0.057529999999999998,0.033390000000000003,0.092740000000000003,0.113,0.089719999999999994,0.13478000000000001,0.13253999999999999,0.071300000000000002,0.043110000000000002,0.047399999999999998,0.068989999999999996,0.033329999999999999,0.069690000000000002,0.20877999999999999,0.30636999999999998,0.22464999999999999,0.11856999999999999,0.15240999999999999,0.097229999999999997,0.10527,0.10826,0.094670000000000004,0.069120000000000001,0.091230000000000006,0.072370000000000004,0.059310000000000002,0.07936,0.1177,0.051229999999999998,0.11902,0.07528,0.070720000000000005,0.10143000000000001,0.097769999999999996,0.11259,0.11545,0.12781000000000001,0.10842,0.11650000000000001,0.16442000000000001,0.15937000000000001,0.16705999999999999,0.18357000000000001,0.11860999999999999,0.12443,0.16472000000000001,0.11459999999999999,0.15039,0.15024999999999999,0.10188,0.11733,0.12572,0.14274999999999999,0.16372,0.15690000000000001,0.14632999999999999,0.28550999999999999,0.23838999999999999,0.24248,0.20977000000000001,0.22472,0.25658999999999998,0.14827000000000001,0.26694000000000001,0.18947,0.19699,0.20416000000000001,0.26501999999999998,0.22644,0.17119000000000001,0.26611000000000001,0.24798000000000001,0.19466,0.30191000000000001,0.21615000000000001,0.23216000000000001,0.31019000000000002,0.28605999999999998],"text":["Datum: 2020-02-01<br />value: 0.01485<br />variable: Kritisch","Datum: 2020-02-02<br />value: 0.16394<br />variable: Kritisch","Datum: 2020-02-03<br />value: 0.07823<br />variable: Kritisch","Datum: 2020-02-04<br />value: 0.05409<br />variable: Kritisch","Datum: 2020-02-05<br />value: 0.03826<br />variable: Kritisch","Datum: 2020-02-06<br />value: 0.07207<br />variable: Kritisch","Datum: 2020-02-07<br />value: 0.05753<br />variable: Kritisch","Datum: 2020-02-08<br />value: 0.03339<br />variable: Kritisch","Datum: 2020-02-09<br />value: 0.09274<br />variable: Kritisch","Datum: 2020-02-10<br />value: 0.11300<br />variable: Kritisch","Datum: 2020-02-11<br />value: 0.08972<br />variable: Kritisch","Datum: 2020-02-12<br />value: 0.13478<br />variable: Kritisch","Datum: 2020-02-13<br />value: 0.13254<br />variable: Kritisch","Datum: 2020-02-14<br />value: 0.07130<br />variable: Kritisch","Datum: 2020-02-15<br />value: 0.04311<br />variable: Kritisch","Datum: 2020-02-16<br />value: 0.04740<br />variable: Kritisch","Datum: 2020-02-17<br />value: 0.06899<br />variable: Kritisch","Datum: 2020-02-18<br />value: 0.03333<br />variable: Kritisch","Datum: 2020-02-19<br />value: 0.06969<br />variable: Kritisch","Datum: 2020-02-20<br />value: 0.20878<br />variable: Kritisch","Datum: 2020-02-21<br />value: 0.30637<br />variable: Kritisch","Datum: 2020-02-22<br />value: 0.22465<br />variable: Kritisch","Datum: 2020-02-23<br />value: 0.11857<br />variable: Kritisch","Datum: 2020-02-24<br />value: 0.15241<br />variable: Kritisch","Datum: 2020-02-25<br />value: 0.09723<br />variable: Kritisch","Datum: 2020-02-26<br />value: 0.10527<br />variable: Kritisch","Datum: 2020-02-27<br />value: 0.10826<br />variable: Kritisch","Datum: 2020-02-28<br />value: 0.09467<br />variable: Kritisch","Datum: 2020-02-29<br />value: 0.06912<br />variable: Kritisch","Datum: 2020-03-01<br />value: 0.09123<br />variable: Kritisch","Datum: 2020-03-02<br />value: 0.07237<br />variable: Kritisch","Datum: 2020-03-03<br />value: 0.05931<br />variable: Kritisch","Datum: 2020-03-04<br />value: 0.07936<br />variable: Kritisch","Datum: 2020-03-05<br />value: 0.11770<br />variable: Kritisch","Datum: 2020-03-06<br />value: 0.05123<br />variable: Kritisch","Datum: 2020-03-07<br />value: 0.11902<br />variable: Kritisch","Datum: 2020-03-08<br />value: 0.07528<br />variable: Kritisch","Datum: 2020-03-09<br />value: 0.07072<br />variable: Kritisch","Datum: 2020-03-10<br />value: 0.10143<br />variable: Kritisch","Datum: 2020-03-11<br />value: 0.09777<br />variable: Kritisch","Datum: 2020-03-12<br />value: 0.11259<br />variable: Kritisch","Datum: 2020-03-13<br />value: 0.11545<br />variable: Kritisch","Datum: 2020-03-14<br />value: 0.12781<br />variable: Kritisch","Datum: 2020-03-15<br />value: 0.10842<br />variable: Kritisch","Datum: 2020-03-16<br />value: 0.11650<br />variable: Kritisch","Datum: 2020-03-17<br />value: 0.16442<br />variable: Kritisch","Datum: 2020-03-18<br />value: 0.15937<br />variable: Kritisch","Datum: 2020-03-19<br />value: 0.16706<br />variable: Kritisch","Datum: 2020-03-20<br />value: 0.18357<br />variable: Kritisch","Datum: 2020-03-21<br />value: 0.11861<br />variable: Kritisch","Datum: 2020-03-22<br />value: 0.12443<br />variable: Kritisch","Datum: 2020-03-23<br />value: 0.16472<br />variable: Kritisch","Datum: 2020-03-24<br />value: 0.11460<br />variable: Kritisch","Datum: 2020-03-25<br />value: 0.15039<br />variable: Kritisch","Datum: 2020-03-26<br />value: 0.15025<br />variable: Kritisch","Datum: 2020-03-27<br />value: 0.10188<br />variable: Kritisch","Datum: 2020-03-28<br />value: 0.11733<br />variable: Kritisch","Datum: 2020-03-29<br />value: 0.12572<br />variable: Kritisch","Datum: 2020-03-30<br />value: 0.14275<br />variable: Kritisch","Datum: 2020-03-31<br />value: 0.16372<br />variable: Kritisch","Datum: 2020-04-01<br />value: 0.15690<br />variable: Kritisch","Datum: 2020-04-02<br />value: 0.14633<br />variable: Kritisch","Datum: 2020-04-03<br />value: 0.28551<br />variable: Kritisch","Datum: 2020-04-04<br />value: 0.23839<br />variable: Kritisch","Datum: 2020-04-05<br />value: 0.24248<br />variable: Kritisch","Datum: 2020-04-06<br />value: 0.20977<br />variable: Kritisch","Datum: 2020-04-07<br />value: 0.22472<br />variable: Kritisch","Datum: 2020-04-08<br />value: 0.25659<br />variable: Kritisch","Datum: 2020-04-09<br />value: 0.14827<br />variable: Kritisch","Datum: 2020-04-10<br />value: 0.26694<br />variable: Kritisch","Datum: 2020-04-11<br />value: 0.18947<br />variable: Kritisch","Datum: 2020-04-12<br />value: 0.19699<br />variable: Kritisch","Datum: 2020-04-13<br />value: 0.20416<br />variable: Kritisch","Datum: 2020-04-14<br />value: 0.26502<br />variable: Kritisch","Datum: 2020-04-15<br />value: 0.22644<br />variable: Kritisch","Datum: 2020-04-16<br />value: 0.17119<br />variable: Kritisch","Datum: 2020-04-17<br />value: 0.26611<br />variable: Kritisch","Datum: 2020-04-18<br />value: 0.24798<br />variable: Kritisch","Datum: 2020-04-19<br />value: 0.19466<br />variable: Kritisch","Datum: 2020-04-20<br />value: 0.30191<br />variable: Kritisch","Datum: 2020-04-21<br />value: 0.21615<br />variable: Kritisch","Datum: 2020-04-22<br />value: 0.23216<br />variable: Kritisch","Datum: 2020-04-23<br />value: 0.31019<br />variable: Kritisch","Datum: 2020-04-24<br />value: 0.28606<br />variable: Kritisch"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(0,186,56,1)","dash":"solid"},"hoveron":"points","name":"Kritisch","legendgroup":"Kritisch","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1580515200,1580601600,1580688e3,1580774400,1580860800,1580947200,1581033600,158112e4,1581206400,1581292800,1581379200,1581465600,1581552e3,1581638400,1581724800,1581811200,1581897600,1581984e3,1582070400,1582156800,1582243200,1582329600,1582416e3,1582502400,1582588800,1582675200,1582761600,1582848e3,1582934400,1583020800,1583107200,1583193600,158328e4,1583366400,1583452800,1583539200,1583625600,1583712e3,1583798400,1583884800,1583971200,1584057600,1584144e3,1584230400,1584316800,1584403200,1584489600,1584576e3,1584662400,1584748800,1584835200,1584921600,1585008e3,1585094400,1585180800,1585267200,1585353600,158544e4,1585526400,1585612800,1585699200,1585785600,1585872e3,1585958400,1586044800,1586131200,1586217600,1586304e3,1586390400,1586476800,1586563200,1586649600,1586736e3,1586822400,1586908800,1586995200,1587081600,1587168e3,1587254400,1587340800,1587427200,1587513600,15876e5,1587686400],"y":[0.80344000000000004,0.67578000000000005,0.76488999999999996,0.71606999999999998,0.72294000000000003,0.76405999999999996,0.89983000000000002,0.74760000000000004,0.72326999999999997,0.66669999999999996,0.77159999999999995,0.72448000000000001,0.66310999999999998,0.65147999999999995,0.63283,0.75722,0.57599999999999996,0.79625999999999997,0.45807999999999999,0.48535,0.46010000000000001,0.53334999999999999,0.73843999999999999,0.45737,0.58935000000000004,0.63444,0.59355000000000002,0.67039000000000004,0.70189000000000001,0.67754999999999999,0.67776000000000003,0.63446000000000002,0.66647000000000001,0.65056000000000003,0.66174999999999995,0.62189000000000005,0.56430000000000002,0.62934999999999997,0.62114000000000003,0.57120000000000004,0.56503000000000003,0.60716000000000003,0.61368,0.59084000000000003,0.55386000000000002,0.56423000000000001,0.62922999999999996,0.54222000000000004,0.58065,0.63217999999999996,0.68435000000000001,0.55857999999999997,0.62156,0.56742000000000004,0.60972000000000004,0.62977000000000005,0.65691999999999995,0.61802000000000001,0.56284999999999996,0.57071000000000005,0.59772999999999998,0.61058000000000001,0.47023999999999999,0.50505999999999995,0.51812999999999998,0.54981999999999998,0.54271000000000003,0.53988000000000003,0.61578999999999995,0.52781,0.59202999999999995,0.57543999999999995,0.54388999999999998,0.45540000000000003,0.50566,0.57682,0.42897000000000002,0.39731,0.48602000000000001,0.39774999999999999,0.41449000000000003,0.46500999999999998,0.28682000000000002,0.30454999999999999],"text":["Datum: 2020-02-01<br />value: 0.80344<br />variable: Affirmativ","Datum: 2020-02-02<br />value: 0.67578<br />variable: Affirmativ","Datum: 2020-02-03<br />value: 0.76489<br />variable: Affirmativ","Datum: 2020-02-04<br />value: 0.71607<br />variable: Affirmativ","Datum: 2020-02-05<br />value: 0.72294<br />variable: Affirmativ","Datum: 2020-02-06<br />value: 0.76406<br />variable: Affirmativ","Datum: 2020-02-07<br />value: 0.89983<br />variable: Affirmativ","Datum: 2020-02-08<br />value: 0.74760<br />variable: Affirmativ","Datum: 2020-02-09<br />value: 0.72327<br />variable: Affirmativ","Datum: 2020-02-10<br />value: 0.66670<br />variable: Affirmativ","Datum: 2020-02-11<br />value: 0.77160<br />variable: Affirmativ","Datum: 2020-02-12<br />value: 0.72448<br />variable: Affirmativ","Datum: 2020-02-13<br />value: 0.66311<br />variable: Affirmativ","Datum: 2020-02-14<br />value: 0.65148<br />variable: Affirmativ","Datum: 2020-02-15<br />value: 0.63283<br />variable: Affirmativ","Datum: 2020-02-16<br />value: 0.75722<br />variable: Affirmativ","Datum: 2020-02-17<br />value: 0.57600<br />variable: Affirmativ","Datum: 2020-02-18<br />value: 0.79626<br />variable: Affirmativ","Datum: 2020-02-19<br />value: 0.45808<br />variable: Affirmativ","Datum: 2020-02-20<br />value: 0.48535<br />variable: Affirmativ","Datum: 2020-02-21<br />value: 0.46010<br />variable: Affirmativ","Datum: 2020-02-22<br />value: 0.53335<br />variable: Affirmativ","Datum: 2020-02-23<br />value: 0.73844<br />variable: Affirmativ","Datum: 2020-02-24<br />value: 0.45737<br />variable: Affirmativ","Datum: 2020-02-25<br />value: 0.58935<br />variable: Affirmativ","Datum: 2020-02-26<br />value: 0.63444<br />variable: Affirmativ","Datum: 2020-02-27<br />value: 0.59355<br />variable: Affirmativ","Datum: 2020-02-28<br />value: 0.67039<br />variable: Affirmativ","Datum: 2020-02-29<br />value: 0.70189<br />variable: Affirmativ","Datum: 2020-03-01<br />value: 0.67755<br />variable: Affirmativ","Datum: 2020-03-02<br />value: 0.67776<br />variable: Affirmativ","Datum: 2020-03-03<br />value: 0.63446<br />variable: Affirmativ","Datum: 2020-03-04<br />value: 0.66647<br />variable: Affirmativ","Datum: 2020-03-05<br />value: 0.65056<br />variable: Affirmativ","Datum: 2020-03-06<br />value: 0.66175<br />variable: Affirmativ","Datum: 2020-03-07<br />value: 0.62189<br />variable: Affirmativ","Datum: 2020-03-08<br />value: 0.56430<br />variable: Affirmativ","Datum: 2020-03-09<br />value: 0.62935<br />variable: Affirmativ","Datum: 2020-03-10<br />value: 0.62114<br />variable: Affirmativ","Datum: 2020-03-11<br />value: 0.57120<br />variable: Affirmativ","Datum: 2020-03-12<br />value: 0.56503<br />variable: Affirmativ","Datum: 2020-03-13<br />value: 0.60716<br />variable: Affirmativ","Datum: 2020-03-14<br />value: 0.61368<br />variable: Affirmativ","Datum: 2020-03-15<br />value: 0.59084<br />variable: Affirmativ","Datum: 2020-03-16<br />value: 0.55386<br />variable: Affirmativ","Datum: 2020-03-17<br />value: 0.56423<br />variable: Affirmativ","Datum: 2020-03-18<br />value: 0.62923<br />variable: Affirmativ","Datum: 2020-03-19<br />value: 0.54222<br />variable: Affirmativ","Datum: 2020-03-20<br />value: 0.58065<br />variable: Affirmativ","Datum: 2020-03-21<br />value: 0.63218<br />variable: Affirmativ","Datum: 2020-03-22<br />value: 0.68435<br />variable: Affirmativ","Datum: 2020-03-23<br />value: 0.55858<br />variable: Affirmativ","Datum: 2020-03-24<br />value: 0.62156<br />variable: Affirmativ","Datum: 2020-03-25<br />value: 0.56742<br />variable: Affirmativ","Datum: 2020-03-26<br />value: 0.60972<br />variable: Affirmativ","Datum: 2020-03-27<br />value: 0.62977<br />variable: Affirmativ","Datum: 2020-03-28<br />value: 0.65692<br />variable: Affirmativ","Datum: 2020-03-29<br />value: 0.61802<br />variable: Affirmativ","Datum: 2020-03-30<br />value: 0.56285<br />variable: Affirmativ","Datum: 2020-03-31<br />value: 0.57071<br />variable: Affirmativ","Datum: 2020-04-01<br />value: 0.59773<br />variable: Affirmativ","Datum: 2020-04-02<br />value: 0.61058<br />variable: Affirmativ","Datum: 2020-04-03<br />value: 0.47024<br />variable: Affirmativ","Datum: 2020-04-04<br />value: 0.50506<br />variable: Affirmativ","Datum: 2020-04-05<br />value: 0.51813<br />variable: Affirmativ","Datum: 2020-04-06<br />value: 0.54982<br />variable: Affirmativ","Datum: 2020-04-07<br />value: 0.54271<br />variable: Affirmativ","Datum: 2020-04-08<br />value: 0.53988<br />variable: Affirmativ","Datum: 2020-04-09<br />value: 0.61579<br />variable: Affirmativ","Datum: 2020-04-10<br />value: 0.52781<br />variable: Affirmativ","Datum: 2020-04-11<br />value: 0.59203<br />variable: Affirmativ","Datum: 2020-04-12<br />value: 0.57544<br />variable: Affirmativ","Datum: 2020-04-13<br />value: 0.54389<br />variable: Affirmativ","Datum: 2020-04-14<br />value: 0.45540<br />variable: Affirmativ","Datum: 2020-04-15<br />value: 0.50566<br />variable: Affirmativ","Datum: 2020-04-16<br />value: 0.57682<br />variable: Affirmativ","Datum: 2020-04-17<br />value: 0.42897<br />variable: Affirmativ","Datum: 2020-04-18<br />value: 0.39731<br />variable: Affirmativ","Datum: 2020-04-19<br />value: 0.48602<br />variable: Affirmativ","Datum: 2020-04-20<br />value: 0.39775<br />variable: Affirmativ","Datum: 2020-04-21<br />value: 0.41449<br />variable: Affirmativ","Datum: 2020-04-22<br />value: 0.46501<br />variable: Affirmativ","Datum: 2020-04-23<br />value: 0.28682<br />variable: Affirmativ","Datum: 2020-04-24<br />value: 0.30455<br />variable: Affirmativ"],"type":"scatter","mode":"lines","line":{"width":1.8897637795275593,"color":"rgba(97,156,255,1)","dash":"solid"},"hoveron":"points","name":"Affirmativ","legendgroup":"Affirmativ","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.762557077625573,"r":7.3059360730593621,"b":40.182648401826498,"l":48.949771689497723},"plot_bgcolor":"rgba(235,235,235,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724},"title":{"text":"Automatische Kategorisierung von QAnon-Tweets","font":{"color":"rgba(0,0,0,1)","family":"","size":17.534246575342465},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[1580156640,1588044960],"tickmode":"array","ticktext":["Feb","Mar","Apr"],"tickvals":[1580515200,1583020800,1585699200],"categoryorder":"array","categoryarray":["Feb","Mar","Apr"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.6529680365296811,"tickwidth":0.66417600664176002,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.68949771689498},"tickangle":0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176002,"zeroline":false,"anchor":"y","title":{"text":"Datum","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.029399000000000002,0.944079],"tickmode":"array","ticktext":["0.00","0.25","0.50","0.75"],"tickvals":[0,0.25,0.49999999999999994,0.75],"categoryorder":"array","categoryarray":["0.00","0.25","0.50","0.75"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.6529680365296811,"tickwidth":0.66417600664176002,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.68949771689498},"tickangle":0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176002,"zeroline":false,"anchor":"x","title":{"text":"Prozentualer Anteil","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","layer":"below","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.8897637795275593,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.68949771689498},"title":{"text":"Kategorien","font":{"color":"rgba(0,0,0,1)","family":"","size":14.611872146118724}}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"fa1728a1d5b5":{"x":{},"y":{},"colour":{},"type":"scatter"}},"cur_data":"fa1728a1d5b5","visdat":{"fa1728a1d5b5":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script><p>Im Plot wird ersichtlich, dass mit der automatischen Schätzung tendenziell eine Abnahme der affirmativen Verbreitung der QAnon-Verschwörungstheorie bei gleichzeitiger Zunahme der kritischen Stimmen festzustellen ist. Gemäss diesen Daten begann eine kritische Auseinandersetzung während der zunehmenden Verbreitung der neuen Verschwörungstheorie erst mit einer gewissen Verzögerung.</p><h2 id=fazit>Fazit<a hidden class=anchor aria-hidden=true href=#fazit>#</a></h2><p>Es zeigt sich, dass eine insgesamt quantitative Zunahme eines bestimmten verschwörungstheoretischen Begriffs nicht nur affirmativ bzgl. der Verschwörungstheorie sein muss, sondern immer auch Gegenstimmen beinhaltet. Für fundiertere Aussagen müsste jedoch die Datenbasis genauer überprüft werden. Da die Tweets über die Schlagworte “QAnon” und “wwg1wga” gescraped wurden und zweiterer Begriff wohl vor allem von Befürworter:innen als Codewort verwendet wird, sind die Daten auch möglicherweise in Richtung Affirmation hin verzerrt Auch könnte die Wortzuordnung der word embeddings akkurater sein, vielleicht wäre mit einer Textbereinigung ein besseres Resultat erreichbar. Ausserdem hat sich beim Training gezeigt, dass die affirmative Kategorie systematisch unterschätzt wurde. Da müsste genauer hingeschaut werden. Zu untersuchen wäre auch, wie sich die Verteilung im weiteren Verlauf bis heute entwickelt hat. Dies dürfte sich nun als schwieriger erweisen, da Twitter unterdessen gegen die Verbreitung von QAnon-Inhalten <a href=https://www.tagesschau.de/ausland/twitter-verschwoerungstheorien-101.html>vorgeht</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Siehe dazu z.B. <a href=https://www.rnd.de/politik/qanon-der-aufstieg-einer-gefahrlichen-verschworungstheorie-ORTPE4D5YRFRZKVTMJBTFADJTY.html>“QAnon” – der Aufstieg einer gefährlichen Verschwörungstheorie</a>, Redaktionsnetzwerk Deutschland, 1. April 2020, abgerufen am 18. August 2020 oder <a href="https://www.youtube.com/watch?v=9R5TvLCsN-E">Die Verschwörungsfanatiker von QAnon.</a>, Der SPIEGEL, 4. August 2020, abgerufen am 18. August 2020&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://nomaad.github.io/tags/autorit%C3%A4rer-charakter/>Autoritärer Charakter</a></li><li><a href=https://nomaad.github.io/tags/quantitative-textanalyse/>Quantitative Textanalyse</a></li><li><a href=https://nomaad.github.io/tags/rechtsextremismus/>Rechtsextremismus</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nomaad.github.io/>Matthias Zaugg</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>